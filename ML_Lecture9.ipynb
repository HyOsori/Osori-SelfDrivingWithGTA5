{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # 원문\n",
    ">> http://cs231n.github.io/convolutional-networks/\n",
    "\n",
    "# Convolutional Neural Network\n",
    "\n",
    " 합성곱 신경망(Convolutional Neural Network)은 일반 신경망과 매우 유사하다. CNN은 학습 가능한 가중치(Weight)와 편향(Bias)로 구성되어 있다. 각 뉴런은 입력을 받아 내적 연산(Dot Product)을 한 뒤 선택에 따라, 비선형(Non-Linear) 연산을 한다.\n",
    "\n",
    " 전체 네트워크는 일반 신경망과 마찬가지로 미분 가능한 하나의 스코어 함수(Score Function)을 갖게 된다. (가장 앞에서 원본 이미지(Raw Image)를 읽고, 가장 뒤에서 각 클래스에 대한 점수를 출력). 또한 ConvNet은 마지막 레이어에 (SVM/Softmax와 같은) 손실 함수(Loss Function)을 가지며, 우리가 일반 신경망을 학습시킬 때 사용하던 각종 기법들을 동일하게 적용할 수 있다.\n",
    "\n",
    " ConvNet과 일반 신경망의 차이점은, ConvNet 설계자는 입력 데이터가 이미지라는 가정 덕분에 이미지 데이터가 갖는 특성들을 인코딩 할 수 있다. 이러한 아키텍쳐는 포워드 함수(Forward Function)을 더욱 효과적으로 구현할 수 있고 네트워크를 학습시키는데 필요한 매개 변수(Parameter)의 수를 크게 줄일 수 있게 해준다.\n",
    "\n",
    "## 아키텍쳐 개요\n",
    "\n",
    " 이전 내용에서 보았듯이 신경망은 입력받은 벡터를 일련의 히든 레이어(Hidden Layer) 를 통해 변형(Transform) 시킨다. 각 히든 레이어는 뉴런들로 이뤄져 있으며, 각 뉴런은 앞쪽 레이어(Previous layer)의 모든 뉴런과 연결되어 있다(Fully Connected). 같은 레이어 내에 있는 뉴런들 끼리는 연결이 존재하지 않고 서로 독립적이다. 마지막 Fully-Connected 레이어는 출력 레이어라고 불리며, 분류 문제에서 클래스 점수(Class Score)를 나타낸다.\n",
    "\n",
    " 일반 신경망은 이미지를 다루기에 적절하지 않다. CIFAR-10 데이터의 경우 각 이미지가 32 x 32 x 3 (가로,세로 32, 3개 컬러 채널)로 이뤄져 있어서 첫 번째 히든 레이어 내의 하나의 뉴런의 경우 32 x 32 x 3 = 3072개의 가중치가 필요하지만, 더 큰 이미지를 사용할 경우에는 같은 구조를 이용하는 것이 불가능하다.\n",
    "\n",
    " 예를 들어 200 x 200 x 3의 크기를 가진 이미지는 같은 뉴런에 대해 200 x 200 x 3 = 120,000개의 가중치를 필요로 하기 때문이다. 더욱이, 이런 뉴런이 레이어 내에 여러개 존재하므로 모수의 개수가 크게 증가하게 된다. 이와 같이 Fully-Connectivity는 심한 낭비이며 많은 수의 모수는 곧 오버피팅(Overfitting)으로 귀결된다.\n",
    "\n",
    " ConvNet은 입력이 이미지로 이뤄져 있다는 특징을 살려 좀 더 합리적인 방향으로 아키텍쳐를 구성할 수 있다. 특히 일반 신경망과 달리, ConvNet의 레이어들은 가로, 세로, 깊이의 3개 차원을 갖게 된다. 여기에서 말하는 깊이란 전체 신경망의 깊이가 아니라 활성 볼륨(Activation Volume) 에서의 3번째 차원을 말한다.\n",
    " \n",
    " 예를 들어 CIFAR-10 이미지는 32 x 32 x 3(가로, 세로, 깊이)의 차원을 갖는 입력 활성 볼륨(Activation Volume)이라고 볼 수 있다. 조만간 보겠지만, 하나의 레이어에 위치한 뉴런들은 일반 신경망과는 달리 앞 레이어의 전체 뉴런이 아닌 일부에만 연결이 되어 있다. ConvNet 아키텍쳐는 전체 이미지를 클래스 점수들로 이뤄진 하나의 벡터로 만들어주기 때문에 마지막 출력 레이어는 1 x 1 x 10(10은 CIFAR-10 데이터의 클래스 개수)의 차원을 가지게 된다. 이에 대한 그럼은 아래와 같다:\n",
    "\n",
    "![neural_net2](./resources/imgs/neural_net2.jpeg)\n",
    "  \n",
    "$$일반\\ 3-Layer\\ 신경망$$\n",
    "\n",
    "\n",
    "## ConvNet을 이루는 레이어들\n",
    "\n",
    "위에서 다룬 것과 같이, ConvNet의 각 레이어는 미분 가능한 변환 함수를 통해 하나의 액티베이션 볼륨을 또다른 액티베이션 볼륨으로 변환 (transform) 시킨다. ConvNet 아키텍쳐에서는 크게 컨볼루셔널 레이어, 풀링 레이어, Fully-connected 레이어라는 3개 종류의 레이어가 사용된다. 전체 ConvNet 아키텍쳐는 이 3 종류의 레이어들을 쌓아 만들어진다.\n",
    "\n",
    "### CIFAR-10 데이터를 다루기 위한 간단한 ConvNet\n",
    "\n",
    "$$[INPUT] - ([CONV] - [RELU] - [POOL])* - [FC]$$\n",
    "\n",
    "1. INPUT[32x32x3]: 입력으로 이미지가 가로32, 세로32, 그리고 RGB 채널\n",
    "<br><br>\n",
    "2. CONV[32x32x12]: 레이어는 입력 이미지의 일부 영역과 연결되어 있으며, 이 연결된 영역과 자신의 가중치의 내적 연산(Dot Product)을 계산\n",
    "<br><br>\n",
    "3. RELU[32x32x12]: 레이어는 액티베이션 함수(Activation Function)로 max(0, x)를 각 요소에 적용(이 레이어는 볼륨의 크기를 변화시키지 않음)\n",
    "<br><br>\n",
    "4. POOL[16x16x12]: 레이어는 (가로,세로) 차원에 대해 다운샘플링 (downsampling)을 수행해 와 같이 줄어든 볼륨을 출력한다.\n",
    "<br><br>\n",
    "5. FC[1x1x10]: (Fully-Connected) 레이어는 클래스 점수들을 계산해 결과를 출력, 10개의 숫자들은 10개 카테고리에 대한 각 클래스의 점수에 해당. 레이어의 이름에서 유추 가능하듯, 이 레이어는 이전 볼륨의 모든 요소와 연결 됨\n",
    "\n",
    "이와 같이, ConvNet은 픽셀 값으로 이뤄진 원본 이미지를 각 레이어를 거치며 클래스 점수로 변환(Transform) 시킨다.\n",
    "\n",
    "한 가지 기억할 것은, 어떤 레이어는 모수(Parameter)를 갖지만 어떤 레이어는 모수(Parameter)를 갖지 않는다는 것이다.\n",
    "\n",
    "특히 [CONV]/[FC] 레이어들은 단순히 입력 볼륨만이 아니라 가중치(Weight)와 편향(Bias)을 포함하는 활성 함수(Activation Function)이다.\n",
    "\n",
    "반면 [RELU]/[POOL] 레이어들은 고정된 함수이다. [CONV]/[FC] 레이어의 모수(Parameter)들은 각 이미지에 대한 클래스 점수가 해당 이미지의 레이블과 같아지도록 경사 하강법(Gradient Descent)으로 학습된다.\n",
    "\n",
    "## 요약\n",
    "\n",
    "* ConvNet 아키텍쳐는 여러 레이어를 통해 입력 이미지를 클래스 점수로 변환\n",
    "<br><br>\n",
    "* ConvNet은 [CONV]/[RELU]/[POOL]/[FC] 레이어 등으로 구성\n",
    "<br><br>\n",
    "* 각 레이어는 3차원의 입력 볼륨을 미분 가능 함수를 통해 3차원 출력 볼륨으로 변환\n",
    "<br><br>\n",
    "* 모수(parameter)를 갖는 레이어도 있고, 그렇지 않은 레이어도 있다\n",
    " - 모수 포함　: [FC]/[CONV]\n",
    " - 모수 미포함: [RELU]/[POOL]\n",
    " <br><br>\n",
    "* 초모수(hyper-parameter)를 갖는 레이어도 있고 그렇지 않은 레이어도 있다\n",
    " - 초모수 포함　: [FC]/[CONV]/[POOL]\n",
    " - 초모수 미포함: [RELU]\n",
    " \n",
    "<div style=\"text-align: right\"> 초모수(hyper-parameter): 모수(paramter)의 모수(paramter)</div>\n",
    "\n",
    "### Convolutional Layer\n",
    "\n",
    " 합성곱 레이어는 ConvNet을 이루는 핵심 요소이다. 합성곱 레이어의 출력은 3차원으로 정렬된 뉴런들로 해석될 수 있다.\n",
    "\n",
    " 이제부터는 뉴런들의 **연결성(Connectivity)**, **공간상의 배치** 그리고 **모수 공유(parameter sharing)**에 대한 설명이다.\n",
    "\n",
    "### 개요\n",
    "\n",
    "합성곱 레이어의 모수(parameter)들은 일련의 학습가능한 필터들로 이뤄져 있다. 각 필터는 가로/세로 차원으로는 작지만 깊이(Depth) 차원으로는 전체 깊이를 아우른다.\n",
    "\n",
    " 다음 레이어로 **전달(Forward Pass)**할 때에는 각 필터를 입력 볼륨의 가로/세로 차원으로 슬라이딩(*정확히는 Convolve*) 시키며 2차원의 **활성화 맵(Activation Map)을 생성**한다.\n",
    " \n",
    " 필터를 입력 위로 슬라이딩 시킬 때, 필터와 입력의 요소들 사이의 **내적 연산(Dot Product)**이 이뤄진다.\n",
    " \n",
    " 직관적으로 설명하면, 이 신경망은 입력의 특정 위치의 특정 패턴에 대해 반응하는 **활성 필터(Activate Filter)**를 학습한다.\n",
    " \n",
    " 이런 **활성화 맵(Activation Map)**을 깊이(Depth) 차원을 따라 쌓은 것이 곧 출력 볼륨이 된다.\n",
    " \n",
    " 그러므로 출력 볼륨의 각 요소들은 입력의 작은 영역만을 취급하고, 같은 활성화 맵 내의 뉴런들은, 같은 필터를 적용한 결과이므로, 같은 모수들을 공유한다.\n",
    "\n",
    "### 로컬 연결성 (Local connectivity)\n",
    "\n",
    "이미지와 같은 고차원 입력을 다룰 때에는, 현재 레이어의 한 뉴런을 이전 볼륨의 모든 뉴런들과 연결하는 것이 비 실용적이다. 대신에 우리는 레이어의 각 뉴런을 입력 볼륨의 로컬한 영역(local region)에만 연결할 것이다. 이 영역은 리셉티브 필드 (receptive field)라고 불리는 초모수 (hyperparameter) 이다. 깊이 차원 측면에서는 항상 입력 볼륨의 총 깊이를 다룬다 (가로/세로는 작은 영역을 보지만 깊이는 전체를 본다는 뜻). 공간적 차원 (가로/세로)와 깊이 차원을 다루는 방식이 다르다는 걸 기억하자.\n",
    "\n",
    "---\n",
    "\n",
    "예제 1. 예를 들어 입력 볼륨의 크기가 CIFAR-10의 RGB 이미지와 같이 [32x32x3]일 때,  만약 리셉티브 필드의 크기가 5x5라면, CONV 레이어의 각 뉴런은 입력 볼륨의 [5x5x3] 크기의 영역에 가중치 (weight)를 가하게 된다 (총 5x5x3=75 개 가중치).\n",
    "입력 볼륨 (RGB 이미지)의 깊이가 3이므로 마지막 숫자가 3이 된다는 것을 기억하자.\n",
    "\n",
    "예제 2. 입력 볼륨의 크기가 [16x16x20]이라고 하자. 3x3 크기의 리셉티브 필드를 사용하면 CONV 레이어의 각 뉴런은 입력 볼륨과 3x3x20=180 개의 연결을 갖게 된다. 이번에도 입력 볼륨의 깊이가 20이므로 마지막 숫자가 20이 된다는 것을 기억하자.\n",
    "\n",
    "---\n",
    "\n",
    "![neuron_model](./resources/imgs/neuron_model.jpeg)\n",
    "  \n",
    "입력의 일부 영역에만 연결된다는 점을 제외하고는, 이전 신경망에서 다뤄지던 뉴런들과 똑같이 내적 연산과 비선형 함수로 이뤄진다.\n",
    "\n",
    "### 공간적 배치\n",
    "\n",
    "지금까지는 컨볼루션 레이어의 한 뉴런과 입력 볼륨의 연결에 대해 알아보았다. 그러나 아직 출력 볼륨에 얼마나 많은 뉴런들이 있는지, 그리고 그 뉴런들이 어떤식으로 배치되는지는 다루지 않았다.\n",
    "\n",
    "**3개의 초모수(hyperparameter)들이 출력 볼륨의 크기를 결정**하게 된다. 그 3개 요소는 바로 **깊이(Depth)**, **보폭(Stride), 제로 패딩(Zero-Padding)** 이다.\n",
    "\n",
    "### 합성곱(Convolution)\n",
    "\n",
    "아래는 컨볼루션 레이어를 설명하는 그림이다.\n",
    "\n",
    "3차원 볼륨은 시각화하기 힘드므로 각 행마다 Depth Slice를 하나씩 배치했다.\n",
    "\n",
    "각 볼륨은 입력 볼륨(파란색), 가중치 볼륨(빨간색), 출력 볼륨(녹색)으로 이뤄진다. \n",
    "\n",
    "* 입력크기: **W1=5, H1=5, D1=3**\n",
    "* 파라미터: **K=2, F=3, S=2, P=1**\n",
    "\n",
    "즉, 2개의 [3×33×3] 크기의 필터(Filter, Kernel)가 각각 **stride 2**로 적용된다. \n",
    "\n",
    "그러므로 출력 볼륨의 spatial(크기, 가로/세로)는 **(5 - 3 + 2) / 2 + 1 = 3**이다. \n",
    "\n",
    "제로 패딩 **P=1** 이 적용되어 입력 볼륨의 가장자리가 모두 0으로 되어있다.\n",
    "\n",
    "하이라이트 표시된 입력(파란색)과 필터(빨간색)의 같은 위치의 원소값들이 \n",
    "**elementwise로 곱해진 뒤 하나로 더해**지고 **bias가 더해**지는걸 볼 수 있다.\n",
    "\n",
    "![conv1](./resources/imgs/conv1.gif)\n",
    "\n",
    "![conv2](./resources/imgs/conv2.gif)\n",
    "\n",
    "\n",
    "### Backpropagation\n",
    "\n",
    "컨볼루션 연산의 backward pass 역시 컨볼루션 연산이다(가로/세로가 뒤집어진 필터를 사용한다는 차이점이 존재), 간단한 1차원 예제를 가지고 쉽게 확인해볼 수 있다.\n",
    "\n",
    "### 풀링 레이어 (Pooling Layer)\n",
    "\n",
    " ConvNet 구조 내에 컨볼루션 레이어들 중간중간에 주기적으로 풀링 레이어를 넣는 것이 일반적이다. 풀링 레이어가 하는 일은 네트워크의 파라미터의 개수나 연산량을 줄이기 위해 사이즈를 줄이는 것이다.\n",
    "\n",
    " 이는 **오버피팅을 조절하는 효과**도 가지고 있다. 풀링 레이어는 MAX 연산을 각 Depth Slice에 대해 독립적으로 적용해 크기를 줄인다.\n",
    "\n",
    " 사이즈 2x2와 stride 2의 Max Pooling 기법이 풀링 레이어에 가장 많이 사용된다. 이 경우 MAX 연산은 4개 숫자(같은 depth slice 내의 2x2 영역) 중 최대값을 선택하고, Depth 차원은 변하지 않는다. \n",
    " \n",
    " 각 Depth Slice를 가로/세로축을 따라 1/2로 downsampling해 3/4의 액티베이션은 버리게 된다.\n",
    " \n",
    "풀링 레이어의 특징들은 일반적으로 아래와 같다\n",
    "\n",
    "1. W1×H1×D1 사이즈의 입력을 받는다\n",
    "<br><br>\n",
    "2. 3가지 hyperparameter를 필요로 한다.\n",
    " - Spatial extent F\n",
    " - Stride S\n",
    " <br><br>\n",
    "3. W2×H2×D2 사이즈의 볼륨을 만든다\n",
    "$$\n",
    "W_2 = \\frac{(W_1 − F)}{S} + 1 \\\\\n",
    "H_2 = \\frac{(H_1 − F)}{S} + 1 \\\\\n",
    "D2=D1\n",
    "$$\n",
    "<br><br>\n",
    "4. 입력에 대해 항상 같은 연산을 하므로 파라미터는 따로 존재하지 않는다\n",
    "5. 풀링 레이어에는 보통 제로 패딩을 하지 않는다\n",
    "\n",
    "일반적으로 실전에서는 두 종류의 max 풀링 레이어만 널리 쓰인다.\n",
    "\n",
    "하나는 overlapping 풀링이라고도 불리는 F=3, S=2이고 \n",
    "하나는 더 자주 쓰이는 F=2,S=2이다.\n",
    "\n",
    "큰 리셉티브 필드에 대해서 풀링을 하면 보통 너무 많은 정보를 버리게 된다.\n",
    "\n",
    "### 일반적인 Pooling\n",
    "\n",
    "1. Max Pooling\n",
    "2. Average Pooling\n",
    "3. L2-Norm Pooling\n",
    "\n",
    "등 다른 연산으로 풀링이 가능하며, Average Pooling은 Max Pooling보다 성능이 떨어져 거의 사용되지 않는 추세다.\n",
    "\n",
    "### 최근 추세\n",
    "\n",
    "1. Fractional Max-Pooling\n",
    "<br>2x2보다 더 작은 필터들로 풀링하는 방식. 1x1, 1x2, 2x1, 2x2 크기의 필터들을 임의로 조합해 풀링한다. 매 forward pass마다 grid들이 랜덤하게 생성되고, 테스트 때에는 여러 grid들의 예측 점수들의 평균치를 사용하게 된다.\n",
    "<br><br>\n",
    "2. Striving for Simplicity\n",
    "<br>The All Convolutional Net 라는 논문은 컨볼루션 레이어만 반복하며 풀링 레이어를 사용하지 않는 방식을 제안한다. Representation의 크기를 줄이기 위해 가끔씩 큰 stride를 가진 컨볼루션 레이어를 사용한다.\n",
    "<br>풀링 레이어가 보통 Representation의 크기를 심하게 줄이기 때문에 (이런 효과는 작은 데이터셋에서만 오버피팅 방지 효과 등으로 인해 도움이 됨), 최근 추세는 점점 풀링 레이어를 사용하지 않는 쪽으로 발전하고 있다.\n",
    "\n",
    "### Normalization 레이어\n",
    "\n",
    "실제 두뇌의 억제 메커니즘 구현 등을 위해 많은 종류의 normalization 레이어들이 제안되었다.\n",
    "\n",
    "그러나 이런 레이어들이 실제로 주는 효과가 별로 없다는 것이 알려지면서 최근에는 거의 사용되지 않고 있다.\n",
    "\n",
    "### Fully Connected 레이어\n",
    "\n",
    "Fully Connected 레이어 내의 뉴런들은 이전 레이어의 모든 Activation들과 연결된다.\n",
    "\n",
    "그러므로 Fully Connected 레이어의 Activation y = XW + b 형태로 행렬곱과 덧셈을 통해 구할 수 있다.\n",
    "\n",
    "### 레이어 크기 결정 패턴\n",
    "\n",
    "ConvNet 구조의 크기를 결정하는 법칙(수학적으로 증명된 법칙은 아니고 실험적으로 좋은 법칙):\n",
    "\n",
    "**INPUT 레이어** (이미지 포함)는 여러번 2로 나눌 수 있어야 한다. 흔히 사용되는 숫자들은 32 (CIFAR-10 데이터), 64, 96 (STL-10), 224 (많이 쓰이는 ImageNet ConvNet), 384, 512 등이 있다.\n",
    "\n",
    "**CONV 레이어**는 (3x3 또는 최대 5x5의)작은 필터들과 S=1의 stride를 사용하며, 결정적으로 입력과 출력의 spatial 크기 (가로/세로)가 달라지지 않도록 입력 볼륨에 제로 패딩을 해 줘야 한다.\n",
    "\n",
    "즉, F=3이라면, P=1로 제로 패딩을 해 주면 입력의 spatial 사이즈를 그대로 유지하게 된다. 만약 F=5라면 P=2를 사용하게 된다. \n",
    "\n",
    "일반적으로 F에 대해서 P=(F−1)/2를 사용하면 입력의 크기가 그대로 유지된다. 만약 7x7과 같이 큰 필터를 사용하는 경우에는 보통 이미지와 바로 연결된 첫 번째 CONV 레이어에만 사용한다.\n",
    "\n",
    "**POOL 레이어**는 Spatial 차원에 대한 **다운샘플링**을 위해 사용된다.\n",
    "\n",
    "가장 일반적인 세팅은 2x2의 리셉티브 필드(**F=2**)를 가진 **max 풀링**이다.\n",
    "\n",
    "이 경우 **입력의 3/4의 활성 값이 버려진다**(가로/세로에 대해 각각 절반으로 다운샘플링 하므로).\n",
    "\n",
    "또 다른 약간 덜 사용되는 세팅은 3x3 리셉티브 필드에 stride를 2로 놓는 것이다.\n",
    "\n",
    "Max 풀링에 3보다 큰 리셉티브 필드를 가지는 경우는 너무 많은 정보를 버리게 되므로 거의 사용되지 않는다. 많은 정보 손실은 곧 성능 하락으로 이어진다.\n",
    "\n",
    "모든 CONV 레이어는 입력의 Spatial 크기를 그대로 유지시키고, POOL 레이어만 Spatial 차원의 다운샘플링을 책임지게 된다.\n",
    "\n",
    "또다른 대안은 CONV 레이어에서 1보다 큰 stride를 사용하거나 제로 패딩 주지 않는 것이다. 이 경우에는 전체 ConvNet이 잘 동작하도록 각 레이어의 입력 볼륨들을 잘 살펴봐야 한다.\n",
    "\n",
    "*왜 CONV 레이어에 stride 1을 사용할까?* 보통 작은 stride가 더 잘 동작한다. 뿐만 아니라, 위에서 언급한 것과 같이 stirde를 1로 놓으면 모든 spatial 다운샘플링을 POOL 레이어에 맡기게 되고 CONV 레이어는 입력 볼륨의 깊이만 변화시키게 된다.\n",
    "\n",
    "*왜 패딩을 사용할까?* 앞에서 본 것과 같이 CONV 레이어를 통과하면서 spatial 크기를 그대로 유지하게 해준다는 점 외에도, 패딩을 쓰면 성능도 향상된다. 만약 제로 패딩을 하지 않고 valid convolution (패딩을 하지 않은 convolution)을 한다면 볼륨의 크기는 CONV 레이어를 거칠 때마다 줄어들게 되고, 가장자리의 정보들이 빠르게 사라진다.\n",
    "\n",
    "\n",
    "## 유명 ConvNet\n",
    "\n",
    "필드에서 사용되는 몇몇 ConvNet들은 별명을 갖고 있다.\n",
    "\n",
    "### LeNet\n",
    "최초의 성공적인 ConvNet 애플리케이션들은 1990년대에 Yann LeCun이 만들었다. 그 중에서도 zip 코드나 숫자를 읽는 LeNet 아키텍쳐가 가장 유명하다.\n",
    "\n",
    "### AlexNet\n",
    "컴퓨터 비전 분야에서 ConvNet을 유명하게 만든 것은 Alex Krizhevsky, Ilya Sutskever, Geoff Hinton이 만든 AlexNet이다.\n",
    "\n",
    "AlexNet은 ImageNet ILSVRC challenge 2012에 출전해 2등을 큰 차이로 제치고 1등을 했다 (top 5 에러율 16%, 2등은 26%).\n",
    "\n",
    "아키텍쳐는 LeNet과 기본적으로 유사지만, 더 깊고 크다. 또한 (과거에는 하나의 CONV 레이어 이후에 바로 POOL 레이어를 쌓은 것과 달리) 여러 개의 CONV 레이어들이 쌓여 있다.\n",
    "\n",
    "### ZF Net\n",
    "ILSVRC 2013년의 승자는 Matthew Zeiler와 Rob Fergus가 만들었다. 저자들의 이름을 따 ZFNet이라고 불린다. AlexNet에서 중간 CONV 레이어 크기를 조정하는 등 하이퍼파라미터들을 수정해 만들었다.\n",
    "\n",
    "### GoogLeNet\n",
    "ILSVRC 2014의 승자는 Szegedy et al. 이 구글에서 만들었다. 이 모델의 가장 큰 기여는 파라미터의 개수를 엄청나게 줄여주는 Inception module을 제안한 것이다 (4M, AlexNet의 경우 60M).\n",
    "\n",
    "뿐만 아니라, ConvNet 마지막에 FC 레이어 대신 Average 풀링을 사용해 별로 중요하지 않아 보이는 파라미터들을 많이 줄이게 된다.\n",
    "\n",
    "### VGGNet\n",
    "ILSVRC 2014에서 2등을 한 네트워크는 Karen Simonyan과 Andrew Zisserman이 만든 VGGNet이라고 불리우는 모델이다. 이 모델의 가장 큰 기여는 네트워크의 깊이가 좋은 성능에 있어 매우 중요한 요소라는 것을 보여준 것이다.\n",
    "\n",
    "이들이 제안한 여러 개 모델 중 가장 좋은 것은 16개의 CONV/FC 레이어로 이뤄지며, 모든 컨볼루션은 3x3, 모든 풀링은 2x2만으로 이뤄져 있다. 비록 GoogLeNet보다 이미지 분류 성능은 약간 낮지만, 여러 Transfer Learning 과제에서 더 좋은 성능을 보인다는 것이 나중에 밝혀졌다.\n",
    "\n",
    "그래서 VGGNet은 최근에 이미지 feature 추출을 위해 가장 많이 사용되고 있다. Caffe를 사용하면 Pretrained model을 받아 바로 사용하는 것도 가능하다. VGGNet의 단점은, 매우 많은 메모리를 사용하며 (140M) 많은 연산을 한다는 것이다.\n",
    "\n",
    "### ResNet\n",
    "Kaiming He et al.이 만든 Residual Network가 ILSVRC 2015에서 우승을 차지했다.\n",
    "\n",
    "Skip connection이라는 특이한 구조를 사용하며 **Batch Normalizatoin**을 많이 사용했다는 특징이 있다.\n",
    "\n",
    "이 아키텍쳐는 또한 마지막 부분에서 FC 레이어를 사용하지 않는다. Kaiming의 발표자료 (video, slides)나 Torch로 구현된 최근 실험들 들도 확인할 수 있다.\n",
    "\n",
    "## ConvNet 관련 고려사항들\n",
    "\n",
    "ConvNet을 만들 때 일어나는 가장 큰 병목 현상은 메모리 병목이다. 최신 GPU들은 3GB, 4GB, 6GB, 가장 좋은 GPU들의 경우 12GB의 메모리를 내장하고 있다.\n",
    "\n",
    "1. 중간 단계의 볼륨 크기: 매 레이어에서 발생하는 액티베이션들과 그에 상응하는 그라디언트 (액티베이션과 같은 크기)의 개수이다. 보통 대부분의 액티베이션들은 ConvNet의 앞쪽 레이어들에서 발생된다 (예: 첫 번째 CONV 레이어). 이 값들은 backpropagation에 필요하기 때문에 계속 메모리에 두고 있어야 한다. 학습이 아닌 테스트에만 ConvNet을 사용할 때는 현재 처리 중인 레이어의 액티베이션 값을 제외한 앞쪽 액티베이션들은 버리는 방식으로 구현할 수 있다.\n",
    "<br><br>\n",
    "2. 파라미터 크기: 신경망이 갖고 있는 파라미터의 개수이며, backpropagation을 위한 각 파라미터의 그라디언트, 그리고 최적화에 momentum, Adagrad, RMSProp 등을 사용한다면 이와 관련된 파라미터들도 캐싱해 놓아야 한다. 그러므로 파라미터 저장 공간은 기본적으로 (파라미터 개수의)3배 정도 더 필요하다.\n",
    "<br><br>\n",
    "3. 모든 ConvNet 구현체는 이미지 데이터 배치 등을 위한 기타 용도의 메모리를 유지해야 한다.\n",
    "\n",
    "일단 액티베이션, 그라디언트, 기타용도에 필요한 값들의 개수를 예상했다면, GB 스케일로 바꿔야 한다.\n",
    "\n",
    "예측한 개수에 (floating point가 4바이트, double precision의 경우 8바이트)를 곱한 후, 1024로 여러 번 나눠 KB, MB, GB로 바꾼다.\n",
    "\n",
    "만약 신경망의 크기가 너무 크다면 대부분의 메모리가 액티베이션에 사용되므로, 배치 크기를 줄여 가용 메모리에 맞게 만들어야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3, 3, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1f2a1d1af28>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQ8AAAD8CAYAAABpXiE9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxPCQ1MTwkNTkfwBRARJelRPLdQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f29352b9b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "image = np.array([[[[1],[2],[3]],\n",
    "                   [[4],[5],[6]], \n",
    "                   [[7],[8],[9]]]], dtype=np.float32)\n",
    "print(image.shape)\n",
    "plt.imshow(image.reshape(3,3), cmap='Greys')\n",
    "\n",
    "# (1, 3, 3, 1) => (이미지 개수, 3행, 3열, 하나의 값)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 filter (2,2,1,1) with padding: VALID, SAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image.shape (1, 3, 3, 1)\n",
      "weight.shape (2, 2, 1, 1)\n",
      "conv2d_img.shape (1, 3, 3, 1)\n",
      "[[ 12.  16.   9.]\n",
      " [ 24.  28.  15.]\n",
      " [ 15.  17.   9.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMkAAAC7CAYAAADPLLrPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAACWpJREFUeJzt3X2IZXUdx/H3J1enRavdWmuX9WGN\nFskeIB1HRZAlWdBFXCGD9Y98QBkQpQcK0gKDILH+KJINY0uxiVDDYttkY1G0NErZUdaHdVmdJHBx\nwRxzt0Vbmfr2xz3V9Xpnv7N7fvO7MzufF1zmnHt+M9/fZfhw7jnn3u9RRGBm03vPoCdgNtc5JGYJ\nh8Qs4ZCYJRwSs4RDYpZoFRJJH5T0oKQXm59Lpxn3L0k7mseWNjXNalOb6ySSvge8HhG3SboJWBoR\nX+8z7kBEnNBinmYD0zYku4E1EbFX0grg9xFxep9xDonNW22PST4SEXsBmp8fnmbceyWNS3pc0mUt\na5pVtSgbIOkhYHmfTd88jDqnRMQrkj4KPCzp2Yj4S59ao8Bos3zW0NDQYZSYu44//vhBT6GYycnJ\nQU+hpNci4sRsUJW3Wz2/czfwQETcf6hxixcvjlWrVh3x3OaSkZGRQU+hmLGxsUFPoaQnI2I4G9T2\n7dYW4Kpm+SrgN70DJC2VNNQsLwPOB55vWdesmrYhuQ1YK+lFYG2zjqRhST9txnwcGJf0NPAIcFtE\nOCQ2b6THJIcSEZPAhX2eHweua5b/BHyqTR2zQfIVd7OEQ2KWcEjMEg6JWcIhMUs4JGYJh8Qs4ZCY\nJRwSs4RDYpZwSMwSDolZwiExSzgkZgmHxCzhkJglHBKzRJGQSLpI0m5JE02Tut7tQ5Lua7Y/IWlV\nibpmNbQOiaRjgB8BFwNnAFdIOqNn2LXA3yPiY8APgO+2rWtWS4k9yQgwEREvRcTbwL3A+p4x64Gf\nNcv3AxdKUoHaZrOuREhWAi93re9pnus7JiKmgH3Ah3r/kKTRptPj+NTUVIGpmbVXIiT99gi9He9m\nMoaI2BQRwxExvGhRq0YuZsWUCMke4OSu9ZOAV6YbI2kR8AHg9QK1zWZdiZBsB1ZLOk3SccAGOp0d\nu3V3erwceDh8b2ybJ1q/p4mIKUk3AtuAY4C7ImKnpG8D4xGxBbgT+LmkCTp7kA1t65rVUuSNf0Rs\nBbb2PHdL1/I/gc+XqGVWm6+4myUcErOEQ2KWcEjMEg6JWcIhMUs4JGYJh8Qs4ZCYJRwSs4RDYpZw\nSMwSDolZwiExSzgkZgmHxCxRqznd1ZL+JmlH87iuRF2zGlp/M7GrOd1aOg0ftkvaEhHP9wy9LyJu\nbFvPrLZazenM5q0S33Hv15zunD7jPifpAuAF4CsR8XLvAEmjwCjA8uXLGRsbKzC9wTv77LMHPYVi\n9u/fP+gpFLN58+YZjavVnO63wKqI+DTwEP9vefrOX+pqTrdkyZICUzNrr0pzuoiYjIiDzepPgLMK\n1DWrokpzOkkrulYvBXYVqGtWRa3mdF+UdCkwRac53dVt65rVUqs53c3AzSVqmdXmK+5mCYfELOGQ\nmCUcErOEQ2KWcEjMEg6JWcIhMUs4JGYJh8Qs4ZCYJRwSs4RDYpZwSMwSDolZwiExS5RqTneXpFcl\nPTfNdkm6vWle94ykM0vUNauh1J7kbuCiQ2y/GFjdPEaBOwrVNZt1RUISEY/S+e76dNYDY9HxOLCk\npzmE2ZxV65ikXwO7lZVqm7VSKyQzaWCHpFFJ45LG33jjjQrTMsvVCknawA7cwdHmploh2QJc2Zzl\nOhfYFxF7K9U2a6VI3y1J9wBrgGWS9gDfAo4FiIgf0+nJtQ6YAN4ErilR16yGUs3prki2B3BDiVpm\ntfmKu1nCITFLOCRmCYfELOGQmCUcErOEQ2KWcEjMEg6JWcIhMUs4JGYJh8Qs4ZCYJRwSs4RDYpZw\nSMwSDolZolYHxzWS9kna0TxuKVHXrIYiX9+l08FxIzB2iDGPRcQlheqZVVOrg6PZvFVqTzIT50l6\nmk6/ra9FxM7eAZJG6fQKZvHixdx6660Vpzd7Vq48eppVbt68edBTqK5WSJ4CTo2IA5LWAZvpNM9+\nh4jYBGwCWLp06bs6PJoNQpWzWxGxPyIONMtbgWMlLatR26ytKiGRtFySmuWRpu5kjdpmbdXq4Hg5\ncL2kKeAtYEPTsM5szqvVwXEjnVPEZvOOr7ibJRwSs4RDYpZwSMwSDolZwiExSzgkZgmHxCzhkJgl\nHBKzhENilnBIzBIOiVnCITFLOCRmCYfELNE6JJJOlvSIpF2Sdkr6Up8xknS7pAlJz0g6s21ds1pK\nfDNxCvhqRDwl6X3Ak5IejIjnu8ZcTKc7ymrgHOCO5qfZnNd6TxIReyPiqWb5H8AuoLfR1HpgLDoe\nB5ZIWtG2tlkNRY9JJK0CPgM80bNpJfBy1/oe3h0kJI1KGpc0fvDgwZJTMztixUIi6QTgV8CXI2J/\n7+Y+v/KubikRsSkihiNieGhoqNTUzFop1VX+WDoB+UVE/LrPkD3AyV3rJ9Fpd2o255U4uyXgTmBX\nRHx/mmFbgCubs1znAvsiYm/b2mY1lDi7dT7wBeBZSTua574BnAL/a063FVgHTABvAtcUqGtWReuQ\nRMQf6X/M0T0mgBva1jIbBF9xN0s4JGYJh8Qs4ZCYJRwSs4RDYpZwSMwSDolZwiExSzgkZgmHxCzh\nkJglHBKzhENilnBIzBIOiVmiVnO6NZL2SdrRPG5pW9esllrN6QAei4hLCtQzq6pWczqzeatWczqA\n8yQ9Lel3kj5Rsq7ZbFKnR0OBP9RpTvcH4Du9vbckvR/4d0QckLQO+GFErO7zN0aB0Wb1dGB3kckd\n2jLgtQp1ajhaXkut13FqRJyYDSoSkqY53QPAtkP03uoe/1dgOCIG/g+VNB4Rw4OeRwlHy2uZa6+j\nSnM6ScubcUgaaepOtq1tVkOt5nSXA9dLmgLeAjZEqfd5ZrOsVnO6jcDGtrVmyaZBT6Cgo+W1zKnX\nUezA3exo5Y+lmCUWbEgkXSRpd3Mfx5sGPZ8jJekuSa9Kem7Qc2lrJh9xGoQF+XZL0jHAC8BaOvdO\n2Q5c0eejNHOepAuAA3Rut/fJQc+njeYWgSu6P+IEXDbo/8tC3ZOMABMR8VJEvA3cS+e+jvNORDwK\nvD7oeZQwVz/itFBDMqN7ONrgJB9xqmqhhmRG93C0wUjuv1ndQg2J7+E4R83g/pvVLdSQbAdWSzpN\n0nHABjr3dbQBmuH9N6tbkCGJiCngRmAbnYPDX0bEzsHO6shIugf4M3C6pD2Srh30nFr470ecPtv1\nLdZ1g57UgjwFbHY4FuSexOxwOCRmCYfELOGQmCUcErOEQ2KWcEjMEg6JWeI/TTEFLVhDkXAAAAAA\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f2a320bac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print(\"imag:\\n\", image)\n",
    "print(\"image.shape\", image.shape)\n",
    "weight = tf.constant([[[[1.]],[[1.]]],\n",
    "                      [[[1.]],[[1.]]]])\n",
    "print(\"weight.shape\", weight.shape)\n",
    "#conv2d = tf.nn.conv2d(image, weight, strides=[1, 1, 1, 1], padding='VALID')\n",
    "conv2d = tf.nn.conv2d(image, weight, strides=[1, 1, 1, 1], padding='SAME')\n",
    "conv2d_img = conv2d.eval()\n",
    "print(\"conv2d_img.shape\", conv2d_img.shape)\n",
    "conv2d_img = np.swapaxes(conv2d_img, 0, 3)\n",
    "for i, one_img in enumerate(conv2d_img):\n",
    "    # reshaped_img = one_img.reshape(2, 2)\n",
    "    reshaped_img = one_img.reshape(3, 3)\n",
    "    print(reshaped_img)\n",
    "    plt.subplot(1, 2, i+1), plt.imshow(reshaped_img, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Filters (2, 2, 1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image.shape (1, 3, 3, 1)\n",
      "weight.shape (2, 2, 1, 3)\n",
      "conv2d_img.shape (1, 3, 3, 3)\n",
      "[[ 12.  16.   9.]\n",
      " [ 24.  28.  15.]\n",
      " [ 15.  17.   9.]]\n",
      "[[ 120.  160.   90.]\n",
      " [ 240.  280.  150.]\n",
      " [ 150.  170.   90.]]\n",
      "[[-12. -16.  -9.]\n",
      " [-24. -28. -15.]\n",
      " [-15. -17.  -9.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAACFCAYAAAB7VhJ6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhJMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f2a3235080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print(\"imag:\\n\", image)\n",
    "print(\"image.shape\", image.shape)\n",
    "\n",
    "weight = tf.constant([[[[1.,10.,-1.]],[[1.,10.,-1.]]],\n",
    "                      [[[1.,10.,-1.]],[[1.,10.,-1.]]]])\n",
    "print(\"weight.shape\", weight.shape)\n",
    "conv2d = tf.nn.conv2d(image, weight, strides=[1, 1, 1, 1], padding='SAME')\n",
    "conv2d_img = conv2d.eval()\n",
    "print(\"conv2d_img.shape\", conv2d_img.shape)\n",
    "conv2d_img = np.swapaxes(conv2d_img, 0, 3)\n",
    "for i, one_img in enumerate(conv2d_img):\n",
    "    print(one_img.reshape(3,3))\n",
    "    plt.subplot(1,3,i+1), plt.imshow(one_img.reshape(3,3), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAX POOLING with padding: VALID, SAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2, 2, 1)\n",
      "[[[[ 4.]\n",
      "   [ 3.]]\n",
      "\n",
      "  [[ 2.]\n",
      "   [ 1.]]]]\n"
     ]
    }
   ],
   "source": [
    "image = np.array([[[[4],[3]],\n",
    "                    [[2],[1]]]], dtype=np.float32)\n",
    "# pool = tf.nn.max_pool(image, ksize=[1, 2, 2, 1], strides=[1, 1, 1, 1], padding='VALID')\n",
    "pool = tf.nn.mbax_pool(image, ksize=[1, 2, 2, 1], strides=[1, 1, 1, 1], padding='SAME')\n",
    "print(pool.shape)\n",
    "print(pool.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST with CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1f2a9d252e8>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f2a7fee898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = mnist.train.images[0].reshape(28,28)\n",
    "plt.imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "lbl = mnist.train.labels[0]\n",
    "print(lbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n",
      "Tensor(\"Conv2D_11:0\", shape=(1, 14, 14, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "\n",
    "print(img.shape)\n",
    "img = img.reshape(-1, 28, 28, 1)\n",
    "W1 = tf.Variable(tf.random_normal([3, 3, 1, 5], stddev=0.01)) # [3 x 3], 단일 원소 필터, 5개\n",
    "conv2d = tf.nn.conv2d(img, W1, strides=[1, 2, 2, 1], padding='SAME') # 세로 보폭 2, 가로 보폭 2\n",
    "print(conv2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAABcCAYAAAB+6068AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f2c0acd630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "conv2d_img = conv2d.eval()                 # Tensorflow 실행\n",
    "conv2d_img = np.swapaxes(conv2d_img, 0, 3) # 0th <=> 3rd 축 변경\n",
    "for i, one_img in enumerate(conv2d_img):   # 각 필터 별 컨볼루션 결과\n",
    "    plt.subplot(1, 5, i+1) # 1 ~ 5번의 subplot 중 (i + 1)번째 plot을 사용\n",
    "    plt.imshow(one_img.reshape(14,14), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST with MAX POOLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"MaxPool_5:0\", shape=(1, 7, 7, 5), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAABcCAYAAABOZ1+dAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAACfFJREFUeJzt3VtoFGkWB/D/mU638RITMGMSnHhZ\nHO8SWVTEC7gIk9kH8YIPs28+hKig4pMs+CDihVXwYUVBYvBxUBDjBcZxRNQxoDAKM46zm6xG1EgI\ns6NGDKQ36Xj2wVZjqvqrSqq/7v7M/weDpk+lvuPf6mNPpbpaVBVEROSOz/LdABERDQ0HNxGRYzi4\niYgcw8FNROQYDm4iIsdwcBMROYaDm4jIMRzcRESO4eAmInJMkZWdFhVpPB63seuC0dfXh1QqJWG3\nF5GR8hbVP1T18zAblpaWakVFhe1+8u7BgwehM+Fx4q+srEwrKytt9pN3nZ2d6OrqCjVTQg1uEfka\nwD8BxAA0quo/TNvH43FMnTo1zK6d1dbWBhFpRchMRpASEXmIEJlUVFTg6NGjOWorf2pra1/yWPEY\nUiaVlZVobGzMTWd5UldXF3rbwFMlIhIDcAzAXwHMAfA3EZkz7O4+AaqKVCoFMBM//wEzea+/vx8A\nJoPHymDMJIIw57gXA3ioqo9UtRfAKQBr7LZV2JLJJEQEzMRXLzP5oLW1FQD+x2PFg5lEEGZwTwLQ\nPuDrZ+nHPiIi9SJyR0TupF+NfrL6+vog8tGpqMBMctZc4QjM5NWrV3loK7eeP38OAL0DHvLkMkKP\nE2MmwMe5dHV15a4zB4QZ3H4nyz0/QFHVBlVdqKoLi4qs/Myz0BkzyUdDBcCYSWlpaT56yqkMt03W\nQduM9OMECDhWysrK8tFTwQozuJ8BqB7w9RcAOuy044Z4PD74CTniM/HBTACUl5cDQGLAQ8zlLWYS\nQZjB/ROAL0VkmogkAHwD4ILdtgpbcXExVBXMxFeCmXwwc+ZMACjmseLBTCIIPKehqikR2QrgMt5e\nunNSVX8zfU9paSlWr16dsb527drAxiZPnmys9/T0GOuHDx821m/cuBHYQyYigqKiIvT19YXOpLi4\nGNOnT89Yr6+vD1w36DrWmpoaY33Hjh3G+qVLlwJ7CGEGgH8jRCYjQSwWA4CnCPn8KSkpwcKFmc+Y\njB8/PnDNlpYWYz39j0lGFy7kZIaGzoS8Qp2MVtXvAHxnuRenxGIx9Pb2zsh3HwXo/gg/V+vnFTPx\nYCYR8C3vRESO4eAmInIMBzcRkWM4uImIHMPBTUTkGA5uIiLHcHATETnGyk1FRo8ejfnz52es79q1\nK3Af169fN9YXLFhgrJ84ccJYb25uDuwhfUvOrJg0aRL279+fsX7r1q3AfZw8edJYTyQSxvrWrVuN\n9SlTpgT2cPz48cBtwho3bhxWrFiRsf7kyZPAfVRVVRnr7e3txvrdu3cj7T/bysrKsH79+oz1oOMe\neH9jq4xqa2uN9eLiYmN90A3WciKVShn/XBcvXgzcx6pVq4z16upqY33NGvMNDM+fPx/YQ7bwFTcR\nkWM4uImIHMPBTUTkGA5uIiLHcHATETmGg5uIyDEc3EREjrFyHXdnZycOHDgQaR/Xrl0z1leuXGms\n79y501jfvHlzYA/Hjh0L3Castra2wOtAgwRdQ71p0yZj/dSpU8Z6rj+8t7u7Gzdv3oy0j6dPn0b6\n/o0bNxrrly9fjrT/oWpvb8e2bdsi7WPZsmXG+t69e43106dPG+u7d+8O7GHPnj2B2wxFUVERJkyY\nkLEe9PeYDS9evLC+Rlh8xU1E5BgObiIix3BwExE5hoObiMgxHNxERI7h4CYicgwHNxGRY6xcxx3k\nyJEjgdt0d3cb67NnzzbW586da6xfuXIlsIdcOnPmTOA28+bNM9aD7pNcV1dnrIe5/3UulZSUBG4z\nbdo0Y72ystJYP3fu3JB6yrdDhw4FbmO6xzkAqGqkHsaOHRvp+20Ic6wE/V0H3a87Ho8PqSeb+Iqb\niMgxHNxERI7h4CYicgwHNxGRYzi4iYgcw8FNROQYDm4iIsfk5Tru7du3R97HunXrjPWXL18a662t\nrZF7yKYNGzZE3seWLVuM9YkTJxrrjY2NkXvIpqB+AeDevXuR6osWLTLWw9yDuaOjI3CbbDl79mzg\nNkH3ol+yZImxfvDgQWM91/coD6OmpiZwm9evX0daY+nSpZG+P5tCDW4ReQzgNYB+AClVXWizKRck\nk0mIyK9gJoPNZy4ezMSLmUQwlFfcf1HVP6x14iZm4o+5eDETL2YyTDzHTUTkmLCDWwH8ICJ3RaTe\nbwMRqReROyJyJ5VKZa/DApW+L0joTHLbXd5lzGVgJrn+jMs8C5VJPhrLo9DPn66urlz3VtDCnipZ\npqodIjIRwBURaVHVHwduoKoNABoAYPTo0dHuYuOARCKBZDL557CZiMgnn0lai6pmzGVgJjNmzGAm\n4HGCEM+fWbNmjZRcQgn1iltVO9K//g6gCcBim0254N2d+JiJRx/AXAZhJl7MJILAwS0iY0Wk5N3v\nAXwF4L7txgrZmzdv3t8ak5l4fAYwl3eSySTATPwwkwjCnCqpANCUfoVZBOBbVf3ealcFLpVKobe3\nFyLyC5jJYLOYywfp9xMwEy9mEkHg4FbVRwCCr27PsaamJmM9FosZ6/39/cNeO5FIYNSoUejp6Smo\nXBoaGoz1KH/mIfhXtq7JbWtri7yPMWPGGOvl5eXG+v370V4IVlVVAVnM5Pbt25H3EfSmourqamP9\n6tWrkXtAFjMBgObm5sj7WL58ubG+b9++yGtkCy8HJCJyDAc3EZFjOLiJiBzDwU1E5BgObiIix3Bw\nExE5hoObiMgx8u4dgFndqch/ATwZ8FA5gEK/feNQe5yiqp+H3XiEZAIMIRdm4uWTyXDXzDU+f7ys\nZWJlcHsWEblT6DdKz3WPzCT/6w1HPnpkLvlfbzhs9shTJUREjuHgJiJyTK4Gt/kmGoUh1z0yk/yv\nNxz56JG55H+94bDWY07OcRMRUfbwVAkRkWOsDm4R+VpEWkXkoYj83eZaUYjIYxH5VUR+tv25f8wk\n43oFnwsz8WIm/qznoqpW/gMQA9AG4E8AEgB+ATDH1noRe30MoDwH6zATh3NhJsykUHKx+Yp7MYCH\nqvpIVXsBnAKwxuJ6LmAm/piLFzPxYiZpNgf3JADtA75+ln6sECmAH0TkrojUW1yHmfhzJRdm4sVM\n/FnNJcxnTg6X+DxWqJewLFPVDhGZCOCKiLSo6o8W1mEm/lzJhZl4MRN/VnOx+Yr7GYCBH173BYAO\ni+sNm6p2pH/9HUAT3v4vmQ3MxJ8TuTATL2biz3YuNgf3TwC+FJFpIpIA8A2ACxbXGxYRGSsiJe9+\nD+ArANE+ITYzZuKv4HNhJl7MxF8ucrF2qkRVUyKyFcBlvP1p8ElV/c3WehFUAGgSEeBtHt+q6vc2\nFmIm/hzJhZl4MRN/1nPhOyeJiBzDd04SETmGg5uIyDEc3EREjuHgJiJyDAc3EZFjOLiJiBzDwU1E\n5BgObiIix/wfVJhmFSin9h4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f2d6fb1ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pool = tf.nn.max_pool(conv2d, \n",
    "                      ksize=[1, 2, 2, 1], \n",
    "                      strides=[1, 2, 2, 1], \n",
    "                      padding='SAME')\n",
    "print(pool)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "pool_img = pool.eval()\n",
    "pool_img = np.swapaxes(pool_img, 0, 3)\n",
    "\n",
    "for i, one_img in enumerate(pool_img):\n",
    "    plt.subplot(1, 5, i+1)\n",
    "    plt.imshow(one_img.reshape(7, 7), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST & CNN Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Learning started. It takes sometime.\n",
      "Epoch: 0001 cost = 0.322131441\n",
      "Epoch: 0002 cost = 0.086149222\n",
      "Epoch: 0003 cost = 0.063875355\n",
      "Epoch: 0004 cost = 0.052874141\n",
      "Epoch: 0005 cost = 0.045416309\n",
      "Epoch: 0006 cost = 0.038921274\n",
      "Epoch: 0007 cost = 0.033458253\n",
      "Epoch: 0008 cost = 0.028985214\n",
      "Epoch: 0009 cost = 0.025437944\n",
      "Epoch: 0010 cost = 0.021899389\n",
      "Epoch: 0011 cost = 0.019174992\n",
      "Epoch: 0012 cost = 0.016516186\n",
      "Epoch: 0013 cost = 0.014708604\n",
      "Epoch: 0014 cost = 0.013128898\n",
      "Epoch: 0015 cost = 0.011366158\n",
      "Learning Finished!\n",
      "Accuracy: 0.9891\n",
      "Label:  [8]\n",
      "Prediction:  [8]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "tf.set_random_seed(777)  # reproducibility\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset\n",
    "\n",
    "# hyper parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "X_img = tf.reshape(X, [-1, 28, 28, 1])   # img 28x28x1 (black/white)\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# L1 ImgIn shape=(?, 28, 28, 1)\n",
    "W1 = tf.Variable(tf.random_normal([3, 3, 1, 32], stddev=0.01))\n",
    "#    Conv     -> (?, 28, 28, 32)\n",
    "#    Pool     -> (?, 14, 14, 32)\n",
    "L1 = tf.nn.conv2d(X_img, W1, strides=[1, 1, 1, 1], padding='SAME')\n",
    "L1 = tf.nn.relu(L1)\n",
    "L1 = tf.nn.max_pool(L1, ksize=[1, 2, 2, 1],\n",
    "                    strides=[1, 2, 2, 1], padding='SAME')\n",
    "'''\n",
    "Tensor(\"Conv2D:0\", shape=(?, 28, 28, 32), dtype=float32)\n",
    "Tensor(\"Relu:0\", shape=(?, 28, 28, 32), dtype=float32)\n",
    "Tensor(\"MaxPool:0\", shape=(?, 14, 14, 32), dtype=float32)\n",
    "'''\n",
    "\n",
    "# L2 ImgIn shape=(?, 14, 14, 32)\n",
    "W2 = tf.Variable(tf.random_normal([3, 3, 32, 64], stddev=0.01))\n",
    "#    Conv      ->(?, 14, 14, 64)\n",
    "#    Pool      ->(?, 7, 7, 64)\n",
    "L2 = tf.nn.conv2d(L1, W2, strides=[1, 1, 1, 1], padding='SAME')\n",
    "L2 = tf.nn.relu(L2)\n",
    "L2 = tf.nn.max_pool(L2, ksize=[1, 2, 2, 1],\n",
    "                    strides=[1, 2, 2, 1], padding='SAME')\n",
    "L2_flat = tf.reshape(L2, [-1, 7 * 7 * 64])\n",
    "'''\n",
    "Tensor(\"Conv2D_1:0\", shape=(?, 14, 14, 64), dtype=float32)\n",
    "Tensor(\"Relu_1:0\", shape=(?, 14, 14, 64), dtype=float32)\n",
    "Tensor(\"MaxPool_1:0\", shape=(?, 7, 7, 64), dtype=float32)\n",
    "Tensor(\"Reshape_1:0\", shape=(?, 3136), dtype=float32)\n",
    "'''\n",
    "\n",
    "# Final FC 7x7x64 inputs -> 10 outputs\n",
    "W3 = tf.get_variable(\"W3\", shape=[7 * 7 * 64, 10],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b = tf.Variable(tf.random_normal([10]))\n",
    "logits = tf.matmul(L2_flat, W3) + b\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# train my model\n",
    "print('Learning started. It takes sometime.')\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning Finished!')\n",
    "\n",
    "# Test model and check accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('Accuracy:', sess.run(accuracy, feed_dict={\n",
    "      X: mnist.test.images, Y: mnist.test.labels}))\n",
    "\n",
    "# Get one and predict\n",
    "r = random.randint(0, mnist.test.num_examples - 1)\n",
    "print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "print(\"Prediction: \", sess.run(\n",
    "    tf.argmax(logits, 1), feed_dict={X: mnist.test.images[r:r + 1]}))\n",
    "\n",
    "# plt.imshow(mnist.test.images[r:r + 1].\n",
    "#           reshape(28, 28), cmap='Greys', interpolation='nearest')\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
