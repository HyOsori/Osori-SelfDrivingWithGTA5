{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN(Recurrent Neural Network)\n",
    "\n",
    "RNN에 대한 기본적인 아이디어는 순차적인 정보를 처리한다는 데 있다.\n",
    "\n",
    "기존의 신경망 구조에서는 모든 입력과 출력이 각각 독립적이라고 가정했지만, 많은 경우에 이는 옳지 않은 방법이다.\n",
    "\n",
    "한 예로, 문장에서 다음에 나올 단어를 추측하고 싶다면 이전에 나온 단어들을 아는 것이 큰 도움이 될 것이다. \n",
    "\n",
    "Recurrent란 **동일한 Task**를 **한 시퀀스의 모든 요소마다 적용**하고, 출력 결과를 다음 계산 결과의 입력으로 활용하는 것을 의미한다.\n",
    "\n",
    "이론적으로 RNN은 임의의 길이의 시퀀스 정보를 처리할 수 있지만, 실제로는 비교적 짧은 시퀀스만 효과적으로 처리할 수 있다.\n",
    "\n",
    "![RNN](./resources/imgs/rnn.jpg)\n",
    "\n",
    "> 위 그림은 RNN의 Recurrent를 한 눈에 보여준다. 우리가 관심있는 시퀀스 정보가 5개의 단어로 이루어진 문장이라면, RNN 네트워크는 한 단어당 하나의 layer씩 (Recurrent 연결이 없는, 또는 사이클이 없는) 5-layer 신경망 구조로 펼쳐질 것이다.\n",
    "\n",
    "RNN 구조에서 일어나는 계산에 대한 식은 아래와 같다.\n",
    "\n",
    "$x_t$: 시간 스탭(time step) $t$에서의 입력값\n",
    "\n",
    "$s_t$: 시간 스탭 $t$에서의 hidden state\n",
    " - 이전 시간 스텝의 hiddent state 값과 현재 시간 스텝의 입력값에 의해 계산 됨\n",
    " - $s_t = f(Ux_t + Ws_{t-1})$: 비선형 함수 f는 보통 $tanh$나 $ReLU$가 사용 됨\n",
    " - 첫 hidden state를 계산하기 위한 s$_{-1}$은 보통 $0$으로 초기화\n",
    "\n",
    "$o_t$: 시간 스텝 $t$에서의 출력값(예를 들어, 문장에서 다음 단어를 추측하고 싶다면 단어 수만큼의 차원의 확률 벡터)\n",
    " - $o_t = softmax(Vs_t)$\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hidden state $s_t$는 신경망에서의 **기억 영역**이라고 생각할 수 있다.\n",
    "\n",
    "* $s_t$는 과거의 시간 스텝들에서 일어난 일들에 대한 정보를 전부 담고 있고, \n",
    "\n",
    "* 출력값 $o_t$는 오로지 현재 시간 스텝 t의 메모리에만 의존한다. \n",
    "\n",
    "하지만 위에서 잠깐 언급했듯이, 실제 구현에서는 너무 먼 과거에 일어난 일들은 잘 기억하지 못한다.\n",
    "\n",
    "각 layer 마다의 파라미터 값들이 전부 다 다른 기존의 Deep한 신경망 구조와 달리, \n",
    "\n",
    "RNN은 모든 시간 스텝에 대해 파라미터 값을 전부 공유하고 있다 (위 그림의 U, V, W). \n",
    "\n",
    "이는 RNN이 각 스텝마다 입력값만 다를 뿐 거의 똑같은 계산을 하고 있다는 것을 보여준다. \n",
    "\n",
    "이러한 특성이 RNN에서의 학습해야 할 파라미터 개수를 적은 개수로 줄여준다.\n",
    "\n",
    "---\n",
    "\n",
    "위 다이어그램에서는 매 시간 스텝마다 출력값을 내지만, 문제에 따라 달라질 수도 있다. \n",
    "\n",
    "예를 들어, 문장에서 긍정/부정적인 감정을 추측하고 싶다면 굳이 모든 단어 위치에 대해 \n",
    "\n",
    "추측값을 내지 않고 최종 추측값 하나만 내서 판단하는 것이 더 유용할 수도 있다. \n",
    "\n",
    "---\n",
    "\n",
    "마찬가지로, 입력값 역시 매 시간 스텝마다 꼭 다 필요한 것은 아니다. \n",
    "\n",
    "RNN에서의 핵심은 시퀀스 정보에 대해 어떠한 정보를 추출해 주는 hidden state이기 때문이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 첫째: 어떤 임의의 문장이 올바른 문법의 문장인지 확률 계산하기\n",
    "이는 문장이 문법적으로나 의미적으로 어느 정도 올바른지 측정할 수 있도록 해주고, 보통 자동 번역 시스템의 일부로 활용된다.\n",
    "\n",
    "### 둘째: 새로운 문장 생성하기\n",
    "셰익스피어의 소설에 언어 모델을 학습시키면 셰익스피어가 쓴 글과 비슷한 글을 네트워크가 자동으로 생성해 낸다.\n",
    "RNN 기반의 character-level 언어 모델이 생성해낼 수 있는 여러 가지 재밌는 데모를 [Andrej Karpathy의 블로그 포스트](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)에서 확인할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM(Long short-term memory)\n",
    "\n",
    "여러 종류의 RNN 기법 중에서 가장 많이 사용되는 것은 LSTM으로, 위에서 살펴본 기본 RNN 구조에 비해 \n",
    "\n",
    "더 긴 시퀀스를 효과적으로 잘 기억한다는 장점 때문이다. \n",
    "\n",
    "대략적으로 hidden state를 계산하는 방법만 조금 다를뿐, 위에서 설명한 RNN과 기본적인 구조는 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN을 이용한 여러 NLP(Natural Language Processing) 분야\n",
    "\n",
    "### 언어 모델링과 텍스트 생성\n",
    "\n",
    "언어 모델은 주어진 문장에서 이전 단어들을 보고 다음 단어가 나올 확률을 계산해주는 모델이다. 언어 모델은 어떤 문장이 실제로 존재할 확률이 얼마나 되는지 계산해주기 때문에, 자동 번역의 출력값으로 어떤 문장을 내보내는 것이 더 좋은지 (실생활에서 높은 확률로 존재하는 문장들은 보통 문법적/의미적으로 올바르기 때문) 알려줄 수 있다. 문장에서 다음 단어가 나타날 확률을 계산해주는 주 목적 외의 부수적인 효과로 생성(generative) 모델을 얻을 수 있는데, 출력 확률 분포에서 샘플링을 통해 문장의 다음 단어가 무엇이 되면 좋을지 정한다면 기존에 없던 새로운 문장을 생성할 수 있다. 또한, 학습 데이터에 따라 다양하고 재밌는 여러 가지를 만들어낼 수도 있다. 언어 모델에서의 입력값은 단어들의 시퀀스 (e.g. one-hot encoded 벡터 시퀀스)이고, 출력은 추측된 단어들의 시퀀스이다. 네트워크를 학습할 때에는 시간 스텝 t에서의 출력값이 실제로 다음 입력 단어가 되도록 o_t=x_{t+1}로 정해준다.\n",
    "\n",
    "### 자동 번역 (기계 번역)\n",
    "\n",
    "기계 번역 문제는 입력이 단어들의 시퀀스라는 점에서 언어 모델링과 비슷하지만, 출력값이 다른 언어로 되어있는 단어들의 시퀀스라는 점에서 차이가 있다. 네트워크 상에서의 중요한 차이점은, 입력값을 전부 다 받아들인 다음에서야 네트워크가 출력값을 내보낸다는 점에 있는데, 번역 문제에서는 어순이 다른 문제 등이 있기 때문에 대상 언어의 문장의 첫 단어를 알기 위해선 번역할 문장 전체를 봐야 할 수도 있기 때문이다.\n",
    "\n",
    "\n",
    "### 음성 인식\n",
    "\n",
    "사운드 웨이브의 음향 신호(acoustic signal)를 입력으로 받아들이고, 출력으로는 음소(phonetic segment)들의 시퀀스와 각각의 음소별 확률 분포를 추측할 수 있다.\n",
    "\n",
    "### 이미지 캡션 생성\n",
    "\n",
    "컴퓨터 비젼에서 활발하게 사용된 convolutional neural network(CNN)과 RNN을 함께 사용한다면, 임의의 이미지를 텍스트로 설명해주는 시스템을 만드는 것도 가능하다. 실제로 어떻게 왜 동작하는지는 상당히 신기하다. CNN과 RNN을 합친 모델은 이미지로부터 얻어낸 주요 단어들과 이미지의 각 부분을 매칭해줄 수도 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN 학습하기\n",
    "\n",
    "RNN 네트워크를 학습하는 것은 기존의 신경망 모델 학습과 매우 유사하나,\n",
    "\n",
    "네트워크의 각 시간 스텝마다 파라미터들이 공유되기 때문에 기존의 역전파(Back-Propagation) \n",
    "\n",
    "알고리즘을 그대로 사용하진 못하고 BPTT(Backpropagation Through Time)라는 변형된 알고리즘을 사용한다.\n",
    "\n",
    "그 이유는, 각 출력 부분에서의 Gradient가 현재 시간 스텝에만 의존하지 않고 이전 시간 스텝들에도 의존하기 때문이다.\n",
    "\n",
    "즉, $t = k$에서의 Gradient를 계산하기 위해서는 시간 스텝 k-1개 이전부터의 Gradient 전부를 더해주어야 한다. \n",
    "\n",
    "단순한 RNN을 BPTT로 학습시키는 것은 Vanishing/Exploding Gradient 문제로 인해 긴 시퀀스를 학습시키는 것이 매우 어렵다.\n",
    "\n",
    "이를 해결하기 위한 여러 트릭들이 존재하고, LSTM 등 이 문제를 해결하기 위한 다양한 변종(확장된) RNN 모델들이 연구되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional RNN\n",
    "\n",
    "시간 스텝 t에서의 출력값이 이전 시간 스텝 외에, 이후의 시간 스텝에서 들어오는 입력값에도 영향을 받을 수 있다는 아이디어에 기반한다.\n",
    "\n",
    "예를 들어, 영어 문제에서 빈칸에 가장 알맞는 단어를 채우기 위해서는 빈칸보다 앞쪽 문장들도 봐야겠지만,\n",
    "\n",
    "빈칸 이후의 단어들도 문맥을 파악하는데 도움이 되기 때문이다.\n",
    "\n",
    "네트워크 구조는 RNN에서 단순히 확장되어서, 아래 그림처럼 두 개의 RNN이 동시에 존재하고, \n",
    "\n",
    "출력값은 두 RNN의 hidden state에 모두 의존하도록 계산된다.\n",
    "\n",
    "![bidirectional-rnn](./resources/imgs/bidirectional-rnn.png)\n",
    "\n",
    "\n",
    "### Deep Bidirectional RNN\n",
    "\n",
    "기본적인 Bidirectional RNN의 구조와 비슷하지만, 매 시간 스텝마다 여러 layer가 있다. \n",
    "\n",
    "실제 구현에서는 이러한 구조가 학습할 수 있는 capacity가 크다 (학습 데이터는 훨씬 더 많이 필요하다).\n",
    "\n",
    "![Deep_Bidirectional-rnn](./resources/imgs/Deep_Bidirectional-rnn.png)\n",
    "\n",
    "### LSTM Network\n",
    "\n",
    "LSTM은 RNN과 거의 같은 구조를 갖고 있지만, hidden state를 계산하는 방식이 다르다. \n",
    "\n",
    "LSTM에서는 RNN의 뉴런 대신에 **메모리 셀(Memory Cell)**이라고 불리는 구조를 사용하는데, \n",
    "\n",
    "입력값으로 이전 state $h_{t-1}$와 현재 입력값 $x_t$를 입력으로 받는 블랙박스 형태로 생각하면 된다. \n",
    "\n",
    "메모리 셀 내부에서는 이전 메모리 값을 그대로 남길지 지울지 정하고, \n",
    "\n",
    "현재 state와 메모리 셀의 입력값을 토대로 현재 메모리에 저장할 값을 계산한다. \n",
    "\n",
    "이러한 구조는 긴 시퀀스를 기억하는데 매우 효과적이었다. \n",
    "\n",
    "![LSTM](./resources/imgs/LSTM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# http://www.wildml.com/2016/08/rnns-in-tensorflow-a-practical-guide-and-undocumented-features/\n",
    "# http://learningtensorflow.com/index.html\n",
    "# http://suriyadeepan.github.io/2016-12-31-practical-seq2seq/\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.contrib import rnn\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# One hot encoding for each char in 'hello'\n",
    "h = [1, 0, 0, 0]\n",
    "e = [0, 1, 0, 0]\n",
    "l = [0, 0, 1, 0]\n",
    "o = [0, 0, 0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only 1 Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2\n",
      "array([[[ 1.,  0.,  0.,  0.]]], dtype=float32)\n",
      "array([[[ 0.17490867,  0.44787094]]], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "with tf.variable_scope('one_cell') as scope:\n",
    "    # One cell RNN input_dim (4) -> output_dim (2)\n",
    "    hidden_size = 2\n",
    "    cell = tf.contrib.rnn.BasicRNNCell(num_units=hidden_size)\n",
    "    print(cell.output_size, cell.state_size)\n",
    "\n",
    "    x_data = np.array([[h]], dtype=np.float32) # x_data = [[[1,0,0,0]]]\n",
    "    pp.pprint(x_data)\n",
    "    outputs, _states = tf.nn.dynamic_rnn(cell, x_data, dtype=tf.float32)\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    pp.pprint(outputs.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 sequances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 5, 4)\n",
      "array([[[ 1.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  0.,  1.]]], dtype=float32)\n",
      "array([[[-0.05280573, -0.48137349],\n",
      "        [-0.46984109,  0.56881678],\n",
      "        [-0.89379609, -0.53229475],\n",
      "        [-0.82912683,  0.18403758],\n",
      "        [-0.74849969, -0.39245877]]], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "with tf.variable_scope('two_sequances') as scope:\n",
    "    # One cell RNN input_dim (4) -> output_dim (2). sequence: 5\n",
    "    hidden_size = 2\n",
    "    cell = tf.contrib.rnn.BasicRNNCell(num_units=hidden_size)\n",
    "    x_data = np.array([[h, e, l, l, o]], dtype=np.float32)\n",
    "    print(x_data.shape)\n",
    "    pp.pprint(x_data)\n",
    "    outputs, states = tf.nn.dynamic_rnn(cell, x_data, dtype=tf.float32)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    pp.pprint(outputs.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[[ 1.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  0.,  1.]],\n",
      "\n",
      "       [[ 0.,  1.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  1.],\n",
      "        [ 0.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.]],\n",
      "\n",
      "       [[ 0.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.],\n",
      "        [ 0.,  1.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.]]], dtype=float32)\n",
      "array([[[ 0.05623469, -0.03956522],\n",
      "        [ 0.14818923, -0.06513537],\n",
      "        [ 0.05228928, -0.2019804 ],\n",
      "        [-0.06294766, -0.27175188],\n",
      "        [-0.1737054 , -0.17175992]],\n",
      "\n",
      "       [[ 0.11565009, -0.01944632],\n",
      "        [-0.04699454, -0.05763028],\n",
      "        [-0.12724197, -0.21007976],\n",
      "        [-0.19539118, -0.27607805],\n",
      "        [-0.24900252, -0.3118816 ]],\n",
      "\n",
      "       [[-0.07952952, -0.16230357],\n",
      "        [-0.15622549, -0.2505348 ],\n",
      "        [ 0.00825727, -0.21055229],\n",
      "        [ 0.08274272, -0.18072607],\n",
      "        [-0.0252098 , -0.26102892]]], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "with tf.variable_scope('3_batches') as scope:\n",
    "    # One cell RNN input_dim (4) -> output_dim (2). sequence: 5, batch 3\n",
    "    # 3 batches 'hello', 'eolll', 'lleel'\n",
    "    x_data = np.array([[h, e, l, l, o],\n",
    "                       [e, o, l, l, l],\n",
    "                       [l, l, e, e, l]], dtype=np.float32)\n",
    "    pp.pprint(x_data)\n",
    "    \n",
    "    hidden_size = 2\n",
    "    cell = rnn.BasicLSTMCell(num_units=hidden_size, state_is_tuple=True)\n",
    "    ## https://www.tutorialspoint.com/python/python_tuples.htm\n",
    "    ## * tuple: immutable list\n",
    "    ##\n",
    "    ## if state_is_tuple:\n",
    "    ##     return (c_state, m_state)\n",
    "    ## else:\n",
    "    ##     return c_state + m_state # 결과들을 합쳐서(Concatenate) 하나의 텐서(Tensor)로 반환\n",
    "    \n",
    "    outputs, _states = tf.nn.dynamic_rnn(cell, x_data, dtype=tf.float32)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    pp.pprint(outputs.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Batches with dynamic length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[[ 1.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  0.,  1.]],\n",
      "\n",
      "       [[ 0.,  1.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  1.],\n",
      "        [ 0.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.]],\n",
      "\n",
      "       [[ 0.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.],\n",
      "        [ 0.,  1.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.]]], dtype=float32)\n",
      "array([[[ 0.03836685,  0.01534821],\n",
      "        [-0.02845195,  0.11174238],\n",
      "        [ 0.00475065, -0.00691689],\n",
      "        [ 0.03075164, -0.0851906 ],\n",
      "        [-0.08912348, -0.15213388]],\n",
      "\n",
      "       [[-0.06924959,  0.09345585],\n",
      "        [-0.15079924, -0.05622675],\n",
      "        [-0.111719  , -0.12153181],\n",
      "        [-0.07358058, -0.16840446],\n",
      "        [ 0.        ,  0.        ]],\n",
      "\n",
      "       [[ 0.02656447, -0.08087905],\n",
      "        [ 0.04998814, -0.132653  ],\n",
      "        [-0.01483389,  0.03272988],\n",
      "        [ 0.        ,  0.        ],\n",
      "        [ 0.        ,  0.        ]]], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "with tf.variable_scope('3_batches_dynamic_length') as scope:\n",
    "    # One cell RNN input_dim (4) -> output_dim (5). sequence: 5, batch 3\n",
    "    # 3 batches 'hello', 'eolll', 'lleel'\n",
    "    x_data = np.array([[h, e, l, l, o],\n",
    "                       [e, o, l, l, l],\n",
    "                       [l, l, e, e, l]], dtype=np.float32)\n",
    "    pp.pprint(x_data)\n",
    "    \n",
    "    hidden_size = 2\n",
    "    cell = rnn.BasicLSTMCell(num_units=hidden_size, state_is_tuple=True)\n",
    "    outputs, _states = tf.nn.dynamic_rnn(cell, x_data, dtype=tf.float32, \n",
    "                                         sequence_length=[5,4,3]) # Batch 형태 변환 가능, 미설정 시 x_data.shape로 batch 진행\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    pp.pprint(outputs.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 5, 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial State(Usually Zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[[ 1.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  0.,  1.]],\n",
      "\n",
      "       [[ 0.,  1.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  1.],\n",
      "        [ 0.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.]],\n",
      "\n",
      "       [[ 0.,  0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.],\n",
      "        [ 0.,  1.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.]]], dtype=float32)\n",
      "array([[[ 0.01574663, -0.05222248],\n",
      "        [-0.11916469, -0.18197553],\n",
      "        [-0.1376331 , -0.05894219],\n",
      "        [-0.17311192,  0.09443656],\n",
      "        [-0.20929588,  0.00629247]],\n",
      "\n",
      "       [[-0.12742309, -0.14851737],\n",
      "        [-0.1284361 , -0.16850547],\n",
      "        [-0.13733651, -0.07023881],\n",
      "        [-0.17342094,  0.0849084 ],\n",
      "        [-0.1866931 ,  0.2080057 ]],\n",
      "\n",
      "       [[-0.07609111,  0.12585831],\n",
      "        [-0.11362801,  0.22768313],\n",
      "        [-0.21199527,  0.01591912],\n",
      "        [-0.24682997, -0.11034822],\n",
      "        [-0.18385477,  0.03967865]]], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "with tf.variable_scope('initial_state') as scope:\n",
    "    batch_size = 3\n",
    "    x_data = np.array([[h, e, l, l, o],\n",
    "                      [e, o, l, l, l],\n",
    "                      [l, l, e, e, l]], dtype=np.float32)\n",
    "    pp.pprint(x_data)\n",
    "    \n",
    "    # One cell RNN input_dim (4) -> output_dim (5). sequence: 5, batch: 3\n",
    "    hidden_size=2\n",
    "    cell = rnn.BasicLSTMCell(num_units=hidden_size, state_is_tuple=True)\n",
    "    \n",
    "    '''\n",
    "    cell.zero_state(size, type): cell의 parameter들을 0으로 초기화\n",
    "    '''\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    outputs, _states = tf.nn.dynamic_rnn(cell, x_data,\n",
    "                                         initial_state=initial_state, dtype=tf.float32)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    pp.pprint(outputs.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[[  0.,   1.,   2.],\n",
      "        [  3.,   4.,   5.],\n",
      "        [  6.,   7.,   8.],\n",
      "        [  9.,  10.,  11.],\n",
      "        [ 12.,  13.,  14.]],\n",
      "\n",
      "       [[ 15.,  16.,  17.],\n",
      "        [ 18.,  19.,  20.],\n",
      "        [ 21.,  22.,  23.],\n",
      "        [ 24.,  25.,  26.],\n",
      "        [ 27.,  28.,  29.]],\n",
      "\n",
      "       [[ 30.,  31.,  32.],\n",
      "        [ 33.,  34.,  35.],\n",
      "        [ 36.,  37.,  38.],\n",
      "        [ 39.,  40.,  41.],\n",
      "        [ 42.,  43.,  44.]]], dtype=float32)\n",
      "dynamic rnn:  Tensor(\"MultiRNNCell/rnn/transpose:0\", shape=(3, 5, 5), dtype=float32)\n",
      "array([[[ -1.17925741e-03,  -3.44865985e-04,   8.82364286e-04,\n",
      "           1.03916030e-03,   1.51803193e-03],\n",
      "        [ -4.01360821e-03,  -7.91756087e-04,   3.16292187e-03,\n",
      "           3.16034886e-03,   5.05335489e-03],\n",
      "        [ -6.54326938e-03,   1.57756484e-04,   6.52547227e-03,\n",
      "           4.55375668e-03,   9.17882565e-03],\n",
      "        [ -7.59032276e-03,   3.32919671e-03,   1.07305096e-02,\n",
      "           4.05586977e-03,   1.33002931e-02],\n",
      "        [ -6.92559872e-03,   8.69269390e-03,   1.54685015e-02,\n",
      "           1.29592861e-03,   1.73573364e-02]],\n",
      "\n",
      "       [[ -4.77588110e-05,   2.04808428e-03,   3.50386766e-03,\n",
      "          -5.84544207e-04,   2.35257414e-03],\n",
      "        [  1.18508015e-03,   6.24321960e-03,   8.24596267e-03,\n",
      "          -2.98746908e-03,   5.58941299e-03],\n",
      "        [  3.21963127e-03,   1.15892701e-02,   1.32223694e-02,\n",
      "          -6.92934403e-03,   9.23517067e-03],\n",
      "        [  5.45626879e-03,   1.69299133e-02,   1.74243767e-02,\n",
      "          -1.17806764e-02,   1.26160355e-02],\n",
      "        [  7.29292864e-03,   2.13568844e-02,   2.04340126e-02,\n",
      "          -1.67681165e-02,   1.54072419e-02]],\n",
      "\n",
      "       [[ -7.70765066e-04,   2.11974257e-03,   4.83562704e-03,\n",
      "          -2.71105208e-04,   3.25488625e-03],\n",
      "        [ -6.18202554e-04,   6.04328280e-03,   1.09019792e-02,\n",
      "          -2.20289896e-03,   7.32910354e-03],\n",
      "        [  1.96268611e-05,   1.05008651e-02,   1.66391879e-02,\n",
      "          -5.35620702e-03,   1.13824345e-02],\n",
      "        [  6.37834135e-04,   1.44333532e-02,   2.10777484e-02,\n",
      "          -9.06580593e-03,   1.47133032e-02],\n",
      "        [  8.37282045e-04,   1.72133539e-02,   2.40169391e-02,\n",
      "          -1.26737850e-02,   1.71208773e-02]]], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "# Create input data\n",
    "batch_size=3\n",
    "sequence_length=5\n",
    "input_dim=3\n",
    "\n",
    "x_data = np.arange(45, dtype=np.float32).reshape(batch_size, sequence_length, input_dim)\n",
    "pp.pprint(x_data)  # batch, sequence_length, input_dim\n",
    "\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "with tf.variable_scope('MultiRNNCell') as scope:\n",
    "    # Make rnn\n",
    "    cell = rnn.BasicLSTMCell(num_units=5, state_is_tuple=True)\n",
    "    cell.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    # 3 layers\n",
    "    multi_cell = rnn.MultiRNNCell([copy.copy(cell) for _ in range(3)], state_is_tuple=True)\n",
    "\n",
    "    # rnn in/out\n",
    "    outputs, _states = tf.nn.dynamic_rnn(multi_cell, x_data, dtype=tf.float32)\n",
    "    print(\"dynamic rnn: \", outputs)\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    pp.pprint(outputs.eval())  # batch size, unrolling (time), hidden_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bi-Directional\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(   array([[[  2.02069625e-01,   2.62673907e-02,  -7.80904517e-02,\n",
      "           7.59446621e-02,  -1.19475774e-01],\n",
      "        [  2.64362067e-01,  -1.39325514e-01,  -1.00306101e-01,\n",
      "           1.64778158e-01,  -1.25187844e-01],\n",
      "        [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "           0.00000000e+00,   0.00000000e+00],\n",
      "        [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "           0.00000000e+00,   0.00000000e+00],\n",
      "        [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "           0.00000000e+00,   0.00000000e+00]],\n",
      "\n",
      "       [[  1.41771331e-01,  -2.25216467e-02,  -1.49441831e-07,\n",
      "           1.26625341e-03,  -2.49418407e-03],\n",
      "        [  1.09348319e-01,  -1.24935973e-02,  -1.41997816e-07,\n",
      "           1.69490254e-03,  -1.22597220e-03],\n",
      "        [  8.79141614e-02,  -6.89801155e-03,  -1.28624947e-07,\n",
      "           1.83878455e-03,  -4.75086214e-04],\n",
      "        [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "           0.00000000e+00,   0.00000000e+00],\n",
      "        [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "           0.00000000e+00,   0.00000000e+00]],\n",
      "\n",
      "       [[  4.22481708e-02,  -1.15743780e-03,  -6.85108098e-14,\n",
      "           5.02519561e-06,  -1.81433461e-05],\n",
      "        [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "           0.00000000e+00,   0.00000000e+00],\n",
      "        [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "           0.00000000e+00,   0.00000000e+00],\n",
      "        [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "           0.00000000e+00,   0.00000000e+00],\n",
      "        [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "           0.00000000e+00,   0.00000000e+00]]], dtype=float32),\n",
      "    array([[[ -3.99857491e-01,   4.26760346e-01,  -1.88130587e-01,\n",
      "          -6.29435703e-02,   1.67116493e-01],\n",
      "        [ -2.92937517e-01,   4.32268411e-01,  -3.70993651e-02,\n",
      "          -4.16323058e-02,   1.62406817e-01],\n",
      "        [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "           0.00000000e+00,   0.00000000e+00],\n",
      "        [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "           0.00000000e+00,   0.00000000e+00],\n",
      "        [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "           0.00000000e+00,   0.00000000e+00]],\n",
      "\n",
      "       [[ -2.89202362e-01,   4.54595774e-01,   2.22656727e-02,\n",
      "          -7.49321596e-04,   4.73385267e-02],\n",
      "        [ -2.55272627e-01,   4.37336564e-01,   9.88002960e-03,\n",
      "          -2.22216739e-04,   2.78941244e-02],\n",
      "        [ -2.00985432e-01,   3.79511595e-01,   3.50837107e-03,\n",
      "          -5.49940960e-05,   1.45612955e-02],\n",
      "        [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "           0.00000000e+00,   0.00000000e+00],\n",
      "        [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "           0.00000000e+00,   0.00000000e+00]],\n",
      "\n",
      "       [[ -1.34515062e-01,   3.48959386e-01,   3.60924256e-04,\n",
      "          -1.59369745e-06,   3.07157310e-03],\n",
      "        [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "           0.00000000e+00,   0.00000000e+00],\n",
      "        [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "           0.00000000e+00,   0.00000000e+00],\n",
      "        [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "           0.00000000e+00,   0.00000000e+00],\n",
      "        [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "           0.00000000e+00,   0.00000000e+00]]], dtype=float32))\n",
      "(   LSTMStateTuple(c=array([[  6.59353852e-01,  -6.10004663e-01,  -1.48382902e-01,\n",
      "          2.38520265e-01,  -9.14239764e-01],\n",
      "       [  8.07966352e-01,  -1.00052941e+00,  -1.29023078e-07,\n",
      "          1.84133416e-03,  -2.73853159e+00],\n",
      "       [  8.84249151e-01,  -9.99999523e-01,  -6.85288347e-14,\n",
      "          5.02558760e-06,  -9.84313011e-01]], dtype=float32), h=array([[  2.64362067e-01,  -1.39325514e-01,  -1.00306101e-01,\n",
      "          1.64778158e-01,  -1.25187844e-01],\n",
      "       [  8.79141614e-02,  -6.89801155e-03,  -1.28624947e-07,\n",
      "          1.83878455e-03,  -4.75086214e-04],\n",
      "       [  4.22481708e-02,  -1.15743780e-03,  -6.85108098e-14,\n",
      "          5.02519561e-06,  -1.81433461e-05]], dtype=float32)),\n",
      "    LSTMStateTuple(c=array([[ -8.10950398e-01,   1.13306475e+00,  -4.44350272e-01,\n",
      "         -1.91371456e-01,   3.36739123e-01],\n",
      "       [ -9.66742754e-01,   2.99679756e+00,   7.32458383e-02,\n",
      "         -3.08656283e-02,   9.68998000e-02],\n",
      "       [ -9.98627543e-01,   1.00000000e+00,   1.67265721e-03,\n",
      "         -1.26819219e-03,   7.48596853e-03]], dtype=float32), h=array([[ -3.99857491e-01,   4.26760346e-01,  -1.88130587e-01,\n",
      "         -6.29435703e-02,   1.67116493e-01],\n",
      "       [ -2.89202362e-01,   4.54595774e-01,   2.22656727e-02,\n",
      "         -7.49321596e-04,   4.73385267e-02],\n",
      "       [ -1.34515062e-01,   3.48959386e-01,   3.60924256e-04,\n",
      "         -1.59369745e-06,   3.07157310e-03]], dtype=float32)))\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "with tf.variable_scope('bi-directional') as scope:\n",
    "    # bi-directional rnn\n",
    "    cell_fw = rnn.BasicLSTMCell(num_units=5, state_is_tuple=True)\n",
    "    cell_bw = rnn.BasicLSTMCell(num_units=5, state_is_tuple=True)\n",
    "\n",
    "    outputs, states = tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw, x_data,\n",
    "                                                      sequence_length=[2, 3, 1],\n",
    "                                                      dtype=tf.float32)\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    pp.pprint(sess.run(outputs))\n",
    "    pp.pprint(sess.run(states))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flatten Based SoftMax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[[  0.,   1.,   2.],\n",
      "        [  3.,   4.,   5.],\n",
      "        [  6.,   7.,   8.],\n",
      "        [  9.,  10.,  11.],\n",
      "        [ 12.,  13.,  14.]],\n",
      "\n",
      "       [[ 15.,  16.,  17.],\n",
      "        [ 18.,  19.,  20.],\n",
      "        [ 21.,  22.,  23.],\n",
      "        [ 24.,  25.,  26.],\n",
      "        [ 27.,  28.,  29.]],\n",
      "\n",
      "       [[ 30.,  31.,  32.],\n",
      "        [ 33.,  34.,  35.],\n",
      "        [ 36.,  37.,  38.],\n",
      "        [ 39.,  40.,  41.],\n",
      "        [ 42.,  43.,  44.]]], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "batch_size=3\n",
    "sequence_length=5\n",
    "input_dim=3\n",
    "\n",
    "hidden_size=3\n",
    "\n",
    "num_classes=5\n",
    "\n",
    "x_data = np.arange(45, dtype=np.float32).reshape(batch_size, sequence_length, input_dim)\n",
    "pp.pprint(x_data) # hidden_size=3, sequence_length=4, batch_size=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[  0.,   1.,   2.],\n",
      "       [  3.,   4.,   5.],\n",
      "       [  6.,   7.,   8.],\n",
      "       [  9.,  10.,  11.],\n",
      "       [ 12.,  13.,  14.],\n",
      "       [ 15.,  16.,  17.],\n",
      "       [ 18.,  19.,  20.],\n",
      "       [ 21.,  22.,  23.],\n",
      "       [ 24.,  25.,  26.],\n",
      "       [ 27.,  28.,  29.],\n",
      "       [ 30.,  31.,  32.],\n",
      "       [ 33.,  34.,  35.],\n",
      "       [ 36.,  37.,  38.],\n",
      "       [ 39.,  40.,  41.],\n",
      "       [ 42.,  43.,  44.]], dtype=float32)\n",
      "(15, 3)\n"
     ]
    }
   ],
   "source": [
    "x_data = x_data.reshape(-1, hidden_size)\n",
    "pp.pprint(x_data)\n",
    "print(x_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[  0.,   1.,   2.,   3.,   4.],\n",
      "       [  5.,   6.,   7.,   8.,   9.],\n",
      "       [ 10.,  11.,  12.,  13.,  14.]], dtype=float32)\n",
      "(3, 5)\n"
     ]
    }
   ],
   "source": [
    "softmax_w = np.arange(15, dtype=np.float32).reshape(hidden_size, num_classes)\n",
    "pp.pprint(softmax_w)\n",
    "print(softmax_w.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[[   25.,    28.,    31.,    34.,    37.],\n",
      "        [   70.,    82.,    94.,   106.,   118.],\n",
      "        [  115.,   136.,   157.,   178.,   199.],\n",
      "        [  160.,   190.,   220.,   250.,   280.],\n",
      "        [  205.,   244.,   283.,   322.,   361.]],\n",
      "\n",
      "       [[  250.,   298.,   346.,   394.,   442.],\n",
      "        [  295.,   352.,   409.,   466.,   523.],\n",
      "        [  340.,   406.,   472.,   538.,   604.],\n",
      "        [  385.,   460.,   535.,   610.,   685.],\n",
      "        [  430.,   514.,   598.,   682.,   766.]],\n",
      "\n",
      "       [[  475.,   568.,   661.,   754.,   847.],\n",
      "        [  520.,   622.,   724.,   826.,   928.],\n",
      "        [  565.,   676.,   787.,   898.,  1009.],\n",
      "        [  610.,   730.,   850.,   970.,  1090.],\n",
      "        [  655.,   784.,   913.,  1042.,  1171.]]], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "outputs = np.matmul(x_data, softmax_w) # (15, 3) x (3, 5) = (15, 5)\n",
    "outputs = outputs.reshape(-1, sequence_length, num_classes) # (batch, seq, class) = (?, 5, 5)\n",
    "pp.pprint(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence-to-Sequence, Sequence Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Tensor 'Const:0' shape=(1, 3) dtype=int32>\n",
      "<tf.Tensor 'Const_1:0' shape=(1, 3, 2) dtype=float32>\n",
      "<tf.Tensor 'Const_2:0' shape=(1, 3) dtype=float32>\n",
      "Loss:  0.596759\n"
     ]
    }
   ],
   "source": [
    "# https://tensorflowkorea.gitbooks.io/tensorflow-kr/content/g3doc/tutorials/seq2seq/\n",
    "\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "batch_size = 1\n",
    "sequence_length = 3\n",
    "emb_dim = 2\n",
    "\n",
    "# [batch_size, sequence_length]\n",
    "y_data = tf.constant([[1, 1, 1]]) # [1, 3]\n",
    "pp.pprint(y_data)\n",
    "\n",
    "# [batch_size, sequence_length, emb_dim ]\n",
    "prediction = tf.constant([[[0.2, 0.7], [0.6, 0.2], [0.2, 0.9]]], dtype=tf.float32) # [1, 3, 2]\n",
    "pp.pprint(prediction)\n",
    "\n",
    "# [batch_size, sequence_length]\n",
    "weights = tf.constant([[1, 1, 1]], dtype=tf.float32)\n",
    "pp.pprint(weights)\n",
    "\n",
    "## logits: 예측 결과, one_hot 형태로 표현\n",
    "## targets: 실제 결과\n",
    "## weights: 각 data의 중요도(평등하다면 [1, 1, ... , 1, 1])\n",
    "sequence_loss = tf.contrib.seq2seq.sequence_loss(logits=prediction, targets=y_data, weights=weights)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "## [0.2, 0.7] => softmax_1(y=1): e^(0.7) / (e^(0.7) + e^(0.9)), 'y=0'은 생략\n",
    "## [0.6, 0.2] => softmax_2(y=1): e^(0.2) / (e^(0.6) + e^(0.2)), 'y=0'은 생략\n",
    "## [0.2, 0.9] => softmax_3(y=1): e^(0.9) / (e^(0.2) + e^(0.9)), 'y=0'은 생략\n",
    "\n",
    "## Loss = softmax_cross_entropy\n",
    "## softmax_cross_entropy = sum_i(-log_e(softmax_i(y=1))) / n, (n = 3)\n",
    "\n",
    "## log_10(e) = 0.43429448190325182765108299019028\n",
    "\n",
    "## 1.221403\n",
    "## 2.013753\n",
    "## 3.235155\n",
    "## -log_e(2.013753/3.235155) = 0.474077\n",
    "\n",
    "## 1.822119\n",
    "## 1.221403\n",
    "## 3.043522\n",
    "## -log_e(1.221403/3.043522) = 0.913015\n",
    "\n",
    "## 1.221403\n",
    "## 2.459603\n",
    "## 3.043515\n",
    "## -log_e(2.459603/3.681006) = 0.403186\n",
    "\n",
    "## Loss = (0.474077 + 0.913015 + 0.403186) / 3 = 0.59675933333333333333333333333333\n",
    "\n",
    "print(\"Loss: \", sequence_loss.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss1:  0.513015 Loss2:  0.371101 Loss3:  1.31326 Loss4:  0.646595\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "batch_size = 1\n",
    "sequence_length = 3\n",
    "emb_dim = 2\n",
    "\n",
    "# [batch_size, sequence_length]\n",
    "y_data = tf.constant([[1, 1, 1]])\n",
    "\n",
    "# [batch_size, sequence_length, emb_dim]\n",
    "# 1.3498588075760031039837014871133\n",
    "# 2.0137527074704765216244003137719\n",
    "# 3.3636115150464796256081018008852\n",
    "prediction1 = tf.constant([[[0.3, 0.7], [0.3, 0.7], [0.3, 0.7]]], dtype=tf.float32)\n",
    "prediction2 = tf.constant([[[0.1, 0.9], [0.1, 0.9], [0.1, 0.9]]], dtype=tf.float32)\n",
    "\n",
    "prediction3 = tf.constant([[[1, 0], [1, 0], [1, 0]]], dtype=tf.float32)\n",
    "prediction4 = tf.constant([[[0, 1], [1, 0], [0, 1]]], dtype=tf.float32)\n",
    "\n",
    "# [batch_size, sequence_length]\n",
    "weights = tf.constant([[1, 1, 1]], dtype=tf.float32)\n",
    "\n",
    "sequence_loss1 = tf.contrib.seq2seq.sequence_loss(prediction1, y_data, weights)\n",
    "sequence_loss2 = tf.contrib.seq2seq.sequence_loss(prediction2, y_data, weights)\n",
    "sequence_loss3 = tf.contrib.seq2seq.sequence_loss(prediction3, y_data, weights)\n",
    "sequence_loss4 = tf.contrib.seq2seq.sequence_loss(prediction4, y_data, weights)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print(\"Loss1: \", sequence_loss1.eval(),\n",
    "      \"Loss2: \", sequence_loss2.eval(),\n",
    "      \"Loss3: \", sequence_loss3.eval(),\n",
    "      \"Loss4: \", sequence_loss4.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hello RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss: 1.65122 prediction:  [[3 3 3 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  llllll\n",
      "1 loss: 1.52464 prediction:  [[3 3 3 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  llllll\n",
      "2 loss: 1.44036 prediction:  [[3 3 3 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  llllll\n",
      "3 loss: 1.39887 prediction:  [[3 3 3 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  llllll\n",
      "4 loss: 1.34181 prediction:  [[3 3 3 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  llllll\n",
      "5 loss: 1.25767 prediction:  [[3 3 3 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  lllllo\n",
      "6 loss: 1.17182 prediction:  [[1 3 3 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  illllo\n",
      "7 loss: 1.07741 prediction:  [[1 3 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ilello\n",
      "8 loss: 0.968803 prediction:  [[1 3 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ilello\n",
      "9 loss: 0.86062 prediction:  [[1 3 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ilello\n",
      "10 loss: 0.761278 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "11 loss: 0.658316 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "12 loss: 0.561087 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "13 loss: 0.475869 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "14 loss: 0.394352 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "15 loss: 0.322558 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "16 loss: 0.266238 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "17 loss: 0.220235 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "18 loss: 0.180351 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "19 loss: 0.14724 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "20 loss: 0.120432 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "21 loss: 0.09881 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "22 loss: 0.0813274 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "23 loss: 0.0670147 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "24 loss: 0.0551293 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "25 loss: 0.0452535 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "26 loss: 0.0371655 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "27 loss: 0.0306622 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "28 loss: 0.0254995 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "29 loss: 0.0214207 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "30 loss: 0.0181932 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "31 loss: 0.0156248 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "32 loss: 0.0135643 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "33 loss: 0.0118955 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "34 loss: 0.0105301 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "35 loss: 0.00940133 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "36 loss: 0.0084584 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "37 loss: 0.00766312 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "38 loss: 0.00698618 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "39 loss: 0.00640535 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "40 loss: 0.00590345 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "41 loss: 0.00546697 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "42 loss: 0.0050854 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "43 loss: 0.00475037 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "44 loss: 0.00445499 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "45 loss: 0.00419366 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "46 loss: 0.00396182 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "47 loss: 0.00375547 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "48 loss: 0.00357141 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "49 loss: 0.00340675 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.set_random_seed(777)  # reproducibility\n",
    "\n",
    "idx2char = ['h', 'i', 'e', 'l', 'o']\n",
    "# Teach hello: hihell -> ihello\n",
    "x_data = [[0, 1, 0, 2, 3, 3]]   # hihell\n",
    "x_one_hot = [[[1, 0, 0, 0, 0],   # h 0\n",
    "              [0, 1, 0, 0, 0],   # i 1\n",
    "              [1, 0, 0, 0, 0],   # h 0\n",
    "              [0, 0, 1, 0, 0],   # e 2\n",
    "              [0, 0, 0, 1, 0],   # l 3\n",
    "              [0, 0, 0, 1, 0]]]  # l 3\n",
    "\n",
    "y_data = [[1, 0, 2, 3, 3, 4]]    # ihello\n",
    "\n",
    "num_classes = 5\n",
    "input_dim = 5  # one-hot size\n",
    "hidden_size = 5  # output from the LSTM. 5 to directly predict one-hot\n",
    "batch_size = 1   # one sentence\n",
    "sequence_length = 6  # |ihello| == 6\n",
    "learning_rate = 0.1\n",
    "\n",
    "X = tf.placeholder(\n",
    "    tf.float32, [None, sequence_length, input_dim])  # X one-hot\n",
    "Y = tf.placeholder(tf.int32, [None, sequence_length])  # Y label\n",
    "\n",
    "cell = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_size, state_is_tuple=True)\n",
    "initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "outputs, _states = tf.nn.dynamic_rnn(\n",
    "    cell, X, initial_state=initial_state, dtype=tf.float32)\n",
    "\n",
    "# FC layer\n",
    "X_for_fc = tf.reshape(outputs, [-1, hidden_size])\n",
    "# fc_w = tf.get_variable(\"fc_w\", [hidden_size, num_classes])\n",
    "# fc_b = tf.get_variable(\"fc_b\", [num_classes])\n",
    "# outputs = tf.matmul(X_for_fc, fc_w) + fc_b\n",
    "outputs = tf.contrib.layers.fully_connected(\n",
    "    inputs=X_for_fc, num_outputs=num_classes, activation_fn=None)\n",
    "\n",
    "# reshape out for sequence_loss\n",
    "outputs = tf.reshape(outputs, [batch_size, sequence_length, num_classes])\n",
    "\n",
    "weights = tf.ones([batch_size, sequence_length])\n",
    "sequence_loss = tf.contrib.seq2seq.sequence_loss(\n",
    "    logits=outputs, targets=Y, weights=weights)\n",
    "loss = tf.reduce_mean(sequence_loss)\n",
    "train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "prediction = tf.argmax(outputs, axis=2)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(50):\n",
    "        l, _ = sess.run([loss, train], feed_dict={X: x_one_hot, Y: y_data})\n",
    "        result = sess.run(prediction, feed_dict={X: x_one_hot})\n",
    "        print(i, \"loss:\", l, \"prediction: \", result, \"true Y: \", y_data)\n",
    "\n",
    "        # print char using dic\n",
    "        result_str = [idx2char[c] for c in np.squeeze(result)]\n",
    "        print(\"\\tPrediction str: \", ''.join(result_str))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
