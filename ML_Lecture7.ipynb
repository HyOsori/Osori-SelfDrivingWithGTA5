{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReLU and Weight Initialization\n",
    "\n",
    "## Neural Network with Sigmoid\n",
    "XOR을 기계학습을 통해 구현하기 위해서, 여러 개의 Artificial Neural을 \n",
    "\n",
    "Sigmoid를 이용해 네트워크 형태로 구현하여 성공적으로 해결할 수 있다.\n",
    "\n",
    "$$\n",
    "Sig(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "예제에서는 Sig(x)의 값이 \n",
    "\n",
    "0.5 보다 작은 경우 0 \n",
    "\n",
    "0.5 이거나 큰 경우는 1\n",
    "\n",
    "<hr>\n",
    "\n",
    "[ ] = W[x1, x2] + b\n",
    "\n",
    "(S): Sigmoid\n",
    "\n",
    "```\n",
    "x1 - [\n",
    "          ] -(S)- y1 - [\n",
    "x2 - [\n",
    "                            ] -(S)- y\n",
    "x1 - [\n",
    "          ] -(S)- y2 - [\n",
    "x2 - [\n",
    "```\n",
    "\n",
    "| x1 | x2 | y1 | y2 | y  | xor |\n",
    "| :- | :: | :: | :: | :: | -:  |\n",
    "| 0  | 0  | 0  | 1  | 0  | 0   |\n",
    "| 0  | 1  | 0  | 0  | 1  | 1   |\n",
    "| 1  | 0  | 0  | 0  | 1  | 1   |\n",
    "| 1  | 1  | 1  | 0  | 0  | 0   |\n",
    "\n",
    "## Sigmoid 보다는 ReLU\n",
    "\n",
    "그러나 Sigmoid를 사용해 Layer를 많이 깊게 쌓는 경우 \n",
    "\n",
    "역전파 시에 미분 값이 0에 수렴해서 앞쪽 Layer에는\n",
    "\n",
    "변화가 거의 없게 되는 문제가 발생했다.\n",
    "\n",
    "<hr>\n",
    "\n",
    "이 후, 인공지능 분야는 깊은 침체기에 접어 들게 되는데\n",
    "\n",
    "캐나다의 제프리 힌튼 교수가 ReLU를 사용하면 이러한 문제를\n",
    "\n",
    "해결됨을 보여주는 논문을 발표하면서 인공지능 분야는\n",
    "\n",
    "침체기에서 벗어나게 된다.\n",
    "\n",
    "<hr>\n",
    "\n",
    "ReLU란 Rectified Linear Unit의 약자로\n",
    "\n",
    "매우 단순한 수식으로 표현 가능하다.\n",
    "\n",
    "```\n",
    "ReLU(x) = max(0, x)\n",
    "```\n",
    "\n",
    "0보다 작은 입력 값이 들어오면 비활성화\n",
    "\n",
    "0보다 같거나 큰 입력이 들어오면 그대로 값을 반환\n",
    "\n",
    "하는 함수이다.\n",
    "\n",
    "<hr>\n",
    "\n",
    "ReLU를 사용하면 역전파 시 미분 값이 0에 수렴하는 이른바\n",
    "\n",
    "Vanishing Gradient 문제를 해결 할 수 있다.\n",
    "\n",
    "## Weight Initialization\n",
    "\n",
    "같은 모델을 같은 데이터로 학습 시켜도 다른 결과가 나오는 것을 확인할 수 있는데,\n",
    "\n",
    "이는 모델의 가중치(Weight)를 Random으로 부여하기 때문이다.\n",
    "\n",
    "<hr>\n",
    "\n",
    "또 다른 문제로, Weight가 0으로 초기화 된 Node가 존재하는 경우\n",
    "\n",
    "해당 Node에서 역전파(Back-Propagation)가 이뤄지는 경우 \n",
    "\n",
    "연결된 Node로 전파되는 Gradient 값이 0이 되어 전파가 끊기게 된다.\n",
    " \n",
    "따라서, Node의 **가중치(Weight) 값이 0이 되지 않도록 해야**한다.\n",
    "\n",
    "<hr>\n",
    "\n",
    "제프리 힌튼 교수는 가중치 초기화를 잘 하는 방식으로, \n",
    "\n",
    "**RBM(Restricted Bolzmann Machine)**구조를 사용했는데, \n",
    "\n",
    "이러한 방식을 제프리 힌튼 교수는 **Deep Belief Net**이라고 명명했다.\n",
    "\n",
    "<hr>\n",
    "\n",
    "RBM의 구조는 여러 단으로 쌓인 Layer에서 \n",
    "\n",
    "1. X값만을 이용해 직접 연결된 두 개의 Layer에서\n",
    "\n",
    "2. 입력된 X의 값이 있을 때 임의 Weight로 Encode, \n",
    "\n",
    "3. 결과로 나온 값을 역으로 Decode한 값이 X와 비슷하도록 \n",
    "\n",
    "하는 과정을 **Pre-Training**이라고 한다.\n",
    "\n",
    "이런 RBM 과정을 거쳐 Weight Initialization을 거친 초기 모델로\n",
    "\n",
    "약간의 X값과 Label을 이용해 학습을 하는 것을 **Fine-Tuning**이라고 한다.\n",
    "\n",
    "## Xavier initialization\n",
    "\n",
    "In Python\n",
    "```python\n",
    "W = np.random.randn(fan_in, fan_out) / np.sqrt(fan_in)\n",
    "```\n",
    "\n",
    "## He's initialization\n",
    "\n",
    "In Python\n",
    "```python\n",
    "W = np.random.randn(fan_in, fan_out) / np.sqrt(fan_in / 2)\n",
    "```\n",
    "**np.random.randn(rows, cols)**: 가우시안 표준 정규 분포 난수를 생성\n",
    "\n",
    "## Overfitting 해결법\n",
    "\n",
    "### Regularization\n",
    "\n",
    "$$cost + \\lambda\\sum w^2$$\n",
    "\n",
    "In Python\n",
    "```python\n",
    "l2reg = reg_strength * tf.reduce_sum(tf.square(W))\n",
    "```\n",
    "\n",
    "## Dropout\n",
    "Overfitting을 막는 간단한 방식\n",
    "\n",
    "1. 학습 시에는, 임의로 Node간 연결을 끊어 학습을 시키는 방식\n",
    "![dropout1](resources/imgs/dropout1.png)\n",
    "\n",
    "2. 각 Node들이 결과를 예측하는 전문가라 가정, 몇몇 전문가 없이\n",
    "소수의 전문가들로만 결과를 예측하는 방식으로 학습해보는 것\n",
    "![dropout2](resources/imgs/dropout2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Module\n",
    "\n",
    "### Fast Forward  Module\n",
    "  $$ x - [] - [] - [] - [] - [] - [] - \\hat{y}$$\n",
    "\n",
    "### Split & Merge  Module\n",
    "  $$  ／ [] - [] - [] - [] - [] ＼ $$\n",
    "  $$ x - [] - [] - [] - [] - [] - [] - \\hat{y}$$\n",
    "  $$  ＼ [] - [] - [] - [] - [] ／ $$\n",
    "  \n",
    "### Recurrent Module\n",
    "$$ \n",
    "x1 - [] - [] - [] - []\\\\\n",
    "　|　　|　　|　　|　　\\\\\n",
    "x2 - [] - [] - [] - []\\\\\n",
    "　|　　|　　|　　|　　\\\\\n",
    "\\ 　x3 - [] - [] - [] - []- \\hat{y}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
