{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "Binary Classification에서 선형 Hypothesis로는 Outlier를 반영하기 매우 힘들어져\n",
    "음의 무한대로 가는 경우 0, 양의 무한대로 가는 경우 1의 값을 갖는 방식의 Hypothesis인 **SIGMOID** 함수 채택\n",
    "    \n",
    "$$\n",
    "y = WX + b \\quad => \\quad y = \\frac{1}{1 + e^{-WX}}\n",
    "$$\n",
    "\n",
    "## Logistic Cost Function\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "C(H(x), y) =&\\ y\\ ?\\ -log(H(x)) : -log(1 - H(x))\\\\\n",
    "           =&\\ y(-log(H(x)) + (1-y)(-log(1 - H(x))\n",
    "\\end{align*}\n",
    "$$\n",
    "                \n",
    "## Minimize Cost - Gradient Decent Algorithm\n",
    "$$\n",
    "H(X) = \\frac{1}{1 + e^{-W^{T}X}}\\\\\n",
    "cost(W)=-\\frac{1}{m}\\sum {y} log(H(x)) + {(1-y)}(log(1-H(x)) \\\\\n",
    "W = W - \\alpha\\frac{\\partial}{\\partial W}cost(W)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Lab 5 - Logistic Regression\n",
    "\n",
    "x_data = [[1, 2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]] \n",
    "y_data = [[0], [0], [0], [1], [1], [1]]\n",
    "\n",
    "# placeholders for a tensor that will be always fed.\n",
    "X = tf.placeholder(tf.float32, shape=[None, 2])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([2, 1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "# Hypothesis using sigmoid: tf.div(1., 1. + tf.exp(tf.matmul(X, W)))\n",
    "hypothesis = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "\n",
    "# cost/loss function\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "# Accuracy computation\n",
    "# if hypothesis > 0.5 then True\n",
    "# else False\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "   # Initialize TensorFlow variables\n",
    "   sess.run(tf.global_variables_initializer())\n",
    "\n",
    "   for step in range(10001):\n",
    "       cost_val, _ = sess.run([cost, train], feed_dict={X: x_data, Y: y_data})\n",
    "       if step % 200 == 0:\n",
    "           print(step, cost_val)\n",
    "\n",
    "   # Accuracy report\n",
    "   h, c, a = sess.run([hypothesis, predicted, accuracy],\n",
    "                      feed_dict={X: x_data, Y: y_data})\n",
    "   print(\"\\nHypothesis: \", h, \"\\nCorrect (Y): \", c, \"\\nAccuracy: \", a)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
